{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c32bd7",
   "metadata": {},
   "source": [
    "# Multi-Object Tracking Evaluation Notebook\n",
    "This notebook implements the workflow to evaluate Deep SORT or bytetrack tracking outputs\n",
    "against a manually annotated ground truth (MOTChallenge style).\n",
    "\n",
    "Note: you might need to install motmetrics in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc38c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import motmetrics as mm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "521059ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_name = 'recording_2025-05-14'\n",
    "dataset_name = 'test_ride_niek'\n",
    "\n",
    "GT_CSV    = f'{dataset_name}_reviewed.csv'\n",
    "PRED_CSV  = f'{dataset_name}_tracks_ma30_bytetrack.csv'\n",
    "ANNOT_JSON= f'{dataset_name}_processed_annotations.json'\n",
    "DATA_DIR  = dataset_name\n",
    "ALLOWED_CATEGORIES = [3, 4, 5]  # Categories to consider for evaluation\n",
    "MAX_IOU_DISTANCE = 0.5  # threshold for IoU matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223047c1",
   "metadata": {},
   "source": [
    "## 2. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "babf6228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth(gt_csv):\n",
    "    '''\n",
    "    Load manual annotations and return a DataFrame with columns:\n",
    "    ['image_name','gt_id','object_category'].\n",
    "    Adjusts object_category from 0-4 to 1-5 to match predictions/JSON.\n",
    "    '''\n",
    "    df = pd.read_csv(gt_csv)\n",
    "    df = df.rename(columns={'ID': 'gt_id'})\n",
    "    # Shift object_category by +1 to align 0-4 → 1-5\n",
    "    df['object_category'] = df['object_category'] + 1\n",
    "    return df[['image_name', 'gt_id', 'object_category']]\n",
    "\n",
    "def load_predictions(pred_csv):\n",
    "    \"\"\"\n",
    "    Load tracker outputs and return a DataFrame with columns:\n",
    "    ['image_name','trk_id','object_category','annotation_id'].\n",
    "    Strips any directory prefix by taking basename.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(pred_csv)\n",
    "    df = df.rename(columns={'ID':'trk_id'})\n",
    "    df['image_name'] = df['image_name'].apply(os.path.basename)\n",
    "    return df[['image_name','trk_id','object_category','annotation_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d76ea",
   "metadata": {},
   "source": [
    "## 3. Frame Index Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "919792b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_frame_indices(gt_df, pr_df):\n",
    "    '''\n",
    "    Create a global mapping from image_name to a consecutive frame index,\n",
    "    based on sorted unique image_name values across GT and predictions.\n",
    "    '''\n",
    "    unique_names = sorted(set(gt_df['image_name']).union(pr_df['image_name']))\n",
    "    name_to_frame = {name: idx+1 for idx, name in enumerate(unique_names)}\n",
    "    # Apply mapping\n",
    "    gt_df['frame'] = gt_df['image_name'].map(name_to_frame)\n",
    "    pr_df['frame'] = pr_df['image_name'].map(name_to_frame)\n",
    "    return gt_df, pr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ea8fd",
   "metadata": {},
   "source": [
    "## 4. Fetching Bounding Boxes from COCO Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3c98e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coco_bboxes(ann_json_path, allowed_categories=None):\n",
    "    \"\"\"\n",
    "    Load COCO annotations JSON and return a DataFrame with columns:\n",
    "    ['image_name','x1','y1','x2','y2','category_id','annotation_id'].\n",
    "    Converts normalized bboxes to pixel coords using image width/height.\n",
    "    Strips any directory prefix by taking basename of file_name.\n",
    "    \"\"\"\n",
    "    coco = json.load(open(ann_json_path, 'r'))\n",
    "    img_meta = {img['id']: img for img in coco['images']}\n",
    "    records = []\n",
    "    for ann in coco['annotations']:\n",
    "        cid = ann['category_id']\n",
    "        if allowed_categories and cid not in allowed_categories:\n",
    "            continue\n",
    "        img = img_meta.get(ann['image_id'])\n",
    "        if img is None:\n",
    "            continue\n",
    "        w_img, h_img = img['width'], img['height']\n",
    "        x, y, bw, bh = ann['bbox']  # normalized [0-1]\n",
    "        px, py = int(x * w_img), int(y * h_img)\n",
    "        pw, ph = int(bw * w_img), int(bh * h_img)\n",
    "        fn = os.path.basename(img['file_name'])\n",
    "        records.append({\n",
    "            'image_name': fn,\n",
    "            'x1': px, 'y1': py,\n",
    "            'x2': px + pw, 'y2': py + ph,\n",
    "            'category_id': cid,\n",
    "            'annotation_id': ann['id']\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def attach_bboxes(df, ann_df, key_cols=('image_name','object_category')):\n",
    "    \"\"\"\n",
    "    For each row in df, find all matching ann_df rows on key_cols,\n",
    "    pick one at random (if any), and attach its bbox + annotation_id.\n",
    "    If no match, fill with NaN.\n",
    "    \n",
    "    df must have columns matching key_cols ('object_category' in df \n",
    "    corresponds to 'category_id' in ann_df).\n",
    "    ann_df must have ['image_name','category_id','x1','y1','x2','y2','annotation_id'].\n",
    "    \"\"\"\n",
    "    # rename for uniformity\n",
    "    ann = ann_df.rename(columns={'category_id': 'object_category'})\n",
    "    # build groups\n",
    "    groups = ann.groupby(list(key_cols))\n",
    "\n",
    "    # preallocate columns\n",
    "    out = df.copy()\n",
    "    out[['x1','y1','x2','y2','annotation_id']] = np.nan\n",
    "\n",
    "    # for reproducibility\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "\n",
    "    # for each unique key tuple, sample once and broadcast\n",
    "    for keys, subidx in out.groupby(list(key_cols)).groups.items():\n",
    "        # keys is a tuple (image_name, object_category)\n",
    "        # get candidate bboxes\n",
    "        try:\n",
    "            candidates = groups.get_group(keys)\n",
    "        except KeyError:\n",
    "            # no JSON bbox for this key\n",
    "            continue\n",
    "        # choose one row at random\n",
    "        chosen = candidates.sample(n=1, random_state=rng).iloc[0]\n",
    "        # assign to all matching df rows\n",
    "        mask = (out['image_name'] == keys[0]) & (out['object_category'] == keys[1])\n",
    "        out.loc[mask, ['x1','y1','x2','y2','annotation_id']] = \\\n",
    "            chosen[['x1','y1','x2','y2','annotation_id']].values\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load and prepare data\n",
    "    gt_df = load_ground_truth(GT_CSV)\n",
    "    pr_df = load_predictions(PRED_CSV)\n",
    "    # Check how many rows are in gt_df with no filter and how many with ALLOWED_CATEGORIES filter\n",
    "    print(\"Ground Truth before merging:\", len(gt_df))\n",
    "    print(\"Predictions before merging:\", len(pr_df))\n",
    "    gt_df, pr_df = assign_frame_indices(gt_df, pr_df)\n",
    "    ann_df = load_coco_bboxes(ANNOT_JSON, allowed_categories=ALLOWED_CATEGORIES)\n",
    "    # DEBUG: check what actually landed in ann_df\n",
    "    print(\"JSON categories present:\", sorted(ann_df['category_id'].unique()))\n",
    "    print(\"CSV categories present:\", sorted(gt_df['object_category'].unique()))\n",
    "    print(\"Pred CSV categories present:\", sorted(pr_df['object_category'].unique()))\n",
    "    print(\"Counts by category:\\n\", ann_df['category_id'].value_counts())\n",
    "    # Sum the value counts to verify\n",
    "    print(\"Total annotations loaded:\", len(ann_df))\n",
    "    \n",
    "    gt_counts  = gt_df.groupby('image_name').size().rename('gt_count')\n",
    "    pr_counts  = pr_df.groupby('image_name').size().rename('pred_count')\n",
    "    ann_counts = ann_df.groupby('image_name').size().rename('ann_count')\n",
    "\n",
    "    # 2. Find mismatches (where GT rows ≠ JSON bboxes)\n",
    "    counts_df = pd.concat([gt_counts, pr_counts, ann_counts], axis=1).fillna(0).astype(int)\n",
    "    counts_df['image_name'] = counts_df.index\n",
    "    counts_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 3. Find frames with gt_count > ann_count\n",
    "    gt_gt_ann = counts_df[counts_df['gt_count'] > counts_df['ann_count']]\n",
    "    mismatch_df = counts_df[counts_df['gt_count'] != counts_df['ann_count']]\n",
    "    \n",
    "\n",
    "    print(f\"Frames with GT ≠ ann (total {len(mismatch_df)}):\")\n",
    "    print(f\"Frames with GT > ann (total {len(gt_gt_ann)}):\")\n",
    "    print(gt_gt_ann.sort_values(['ann_count','gt_count'], ascending=False).head(30))\n",
    "    print(mismatch_df.sort_values(['ann_count','gt_count'], ascending=False).head(50))\n",
    "    \n",
    "    print(ann_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe416599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print len of DataFrames to check differences before and after merging\n",
    "print(\"Ground Truth with Frames & BBoxes:\\n\", len(gt_df))\n",
    "print(\"Predictions with Frames & BBoxes:\\n\", len(pr_df))\n",
    "\n",
    "gt_df = attach_bboxes(gt_df, ann_df.rename(columns={'category_id':'object_category'}), \n",
    "                         key_cols=('image_name','object_category'))\n",
    "pr_df = pr_df.merge(ann_df[['annotation_id','x1','y1','x2','y2']], on='annotation_id', how='left')\n",
    "\n",
    "print(\"After merge, GT rows:\", len(gt_df))\n",
    "print(\"   any missing bboxes:\", gt_df[['x1','y1','x2','y2']].isnull().any(axis=1).sum())\n",
    "\n",
    "print(len(gt_df), \"ground truth records loaded.\")\n",
    "print(len(pr_df), \"predictions loaded.\")\n",
    "\n",
    "# Amount of unique objects in GT csv\n",
    "unique_gt_ids = gt_df['gt_id'].nunique()\n",
    "print(f\"Unique GT objects: {unique_gt_ids}\")\n",
    "\n",
    "# Amount of unique objects in predictions\n",
    "unique_pr_ids = pr_df['trk_id'].nunique()\n",
    "print(f\"Unique predicted objects: {unique_pr_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194e8cb7",
   "metadata": {},
   "source": [
    "## 5. Frame-by-Frame Matching and Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093725a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for IoU-based cost\n",
    "from motmetrics.distances import iou_matrix\n",
    "\n",
    "# Initialize accumulator\n",
    "acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "all_ious = []\n",
    "\n",
    "# Iterate over each frame in the ground truth\n",
    "for frame in sorted(gt_df['frame'].unique()):\n",
    "    gt_frame = gt_df[gt_df['frame'] == frame]\n",
    "    pr_frame = pr_df[pr_df['frame'] == frame]\n",
    "\n",
    "    gt_ids = gt_frame['gt_id'].tolist()\n",
    "    pr_ids = pr_frame['trk_id'].tolist()\n",
    "\n",
    "    # Extract bounding boxes in [x1,y1,x2,y2]\n",
    "    gt_boxes = gt_frame[['x1','y1','x2','y2']].values\n",
    "    pr_boxes = pr_frame[['x1','y1','x2','y2']].values\n",
    "\n",
    "    # Compute cost matrix based on IoU\n",
    "    if len(gt_boxes) > 0 and len(pr_boxes) > 0:\n",
    "        # Compute IoU matrix, then cost = 1 - IoU\n",
    "        iou_mat = iou_matrix(gt_boxes, pr_boxes, max_iou=MAX_IOU_DISTANCE) # unmatched pairs get cost > MAX_IOU_DISTANCE\n",
    "        cost_mat = 1 - iou_mat\n",
    "    else:\n",
    "        cost_mat = np.empty((len(gt_boxes), len(pr_boxes)))\n",
    "\n",
    "    # Update accumulator: unmatched rows/cols are handled internally\n",
    "    acc.update(gt_ids, pr_ids, cost_mat)\n",
    "\n",
    "print(\"Accumulation complete. Total frames processed:\", len(sorted(gt_df['frame'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62630b5",
   "metadata": {},
   "source": [
    "## 5.1 Diagnostic: Accumulator Event Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c1340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all accumulator events and print counts by type\n",
    "# In newer motmetrics versions, events are stored in `acc.events`\n",
    "events = getattr(acc, 'events', None)\n",
    "if events is None:\n",
    "    # Fallback: try private attribute\n",
    "    events = acc._events  # DataFrame stored internally\n",
    "print(\"Accumulator Event Types and Counts:\")\n",
    "print(events['Type'].value_counts())\n",
    "print(\"Sample Events (first 10 rows):\")\n",
    "print(events.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check for any missing bounding boxes after merge\n",
    "print(\"Ground truth entries without bboxes:\", gt_df[['x1','y1','x2','y2']].isnull().any(axis=1).sum())\n",
    "print(\"Prediction entries without bboxes:\", pr_df[['x1','y1','x2','y2']].isnull().any(axis=1).sum())\n",
    "\n",
    "# 2. Frames with GT but no predictions (all false negatives)\n",
    "frames_no_pred = sorted(set(gt_df['frame']) - set(pr_df['frame']))\n",
    "print(f\"Frames with GT but no preds: {len(frames_no_pred)} frames (e.g. {frames_no_pred[:5]})\")\n",
    "\n",
    "# 3. Frames with predictions but no GT (all false positives)\n",
    "frames_no_gt = sorted(set(pr_df['frame']) - set(gt_df['frame']))\n",
    "print(f\"Frames with preds but no GT: {len(frames_no_gt)} frames (e.g. {frames_no_gt[:5]})\")\n",
    "\n",
    "# 4. Distribution of object counts per frame\n",
    "cnt_gt = gt_df.groupby('frame').size()\n",
    "cnt_pr = pr_df.groupby('frame').size()\n",
    "print(\"GT objects per frame: min, max, mean =\", cnt_gt.min(), cnt_gt.max(), cnt_gt.mean())\n",
    "print(\"Pred objects per frame: min, max, mean =\", cnt_pr.min(), cnt_pr.max(), cnt_pr.mean())\n",
    "\n",
    "# 5. Sample IoU values to spot mismatches\n",
    "# For a frame that exists in both GT and predictions\n",
    "common_frames = sorted(set(gt_df['frame']).intersection(pr_df['frame']))\n",
    "if common_frames:\n",
    "    sample_frame = common_frames[0]  # pick the first shared frame\n",
    "    gt_sample = gt_df[gt_df.frame==sample_frame][['x1','y1','x2','y2']].values\n",
    "    pr_sample = pr_df[pr_df.frame==sample_frame][['x1','y1','x2','y2']].values\n",
    "    iou_sample = iou_matrix(gt_sample, pr_sample, max_iou=1.0)\n",
    "    print(f\"Sample IoU matrix for frame {sample_frame}:\", np.round(iou_sample,2))\n",
    "else:\n",
    "    print(\"No overlapping frames for IoU sampling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb9c214",
   "metadata": {},
   "source": [
    "## 5.3 Visualize Sample Frame with GT vs. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Determine a sample frame with both GT and predictions\n",
    "common_frames = sorted(set(gt_df['frame']).intersection(pr_df['frame']))\n",
    "if common_frames:\n",
    "    # pick a random common frame\n",
    "    import random\n",
    "    # Randomly select a frame from the common frames\n",
    "    sample_frame = random.choice(common_frames)\n",
    "    # Fetch image filename for this frame\n",
    "    img_name = gt_df[gt_df.frame == sample_frame]['image_name'].iloc[0]\n",
    "    # Build full path to frame (adjust DATA_DIR as needed)\n",
    "    img_path = os.path.join(DATA_DIR, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Could not load image at {img_path}\")\n",
    "    else:\n",
    "        # Draw GT boxes in green\n",
    "        gt_boxes = gt_df[gt_df.frame == sample_frame][['x1','y1','x2','y2']].values\n",
    "        for (x1,y1,x2,y2) in gt_boxes:\n",
    "            cv2.rectangle(img, (int(x1),int(y1)), (int(x2),int(y2)), (0,255,0), 2)\n",
    "        # Draw Pred boxes in red\n",
    "        pr_boxes = pr_df[pr_df.frame == sample_frame][['x1','y1','x2','y2']].values\n",
    "        for (x1,y1,x2,y2) in pr_boxes:\n",
    "            cv2.rectangle(img, (int(x1),int(y1)), (int(x2),int(y2)), (0,0,255), 1)\n",
    "        # Display with matplotlib\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Frame {sample_frame}: GT (green) vs. Pred (red)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        # Print IoU matrix\n",
    "        iou_mat = iou_matrix(gt_boxes, pr_boxes, max_iou=1.0)\n",
    "        print(f\"IoU matrix for frame {sample_frame}:\", np.round(iou_mat,2))\n",
    "else:\n",
    "    print(\"No common frames to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169541e9",
   "metadata": {},
   "source": [
    "## 5. Compute Tracking-Only Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = mm.metrics.create()\n",
    "metrics = ['idf1',    # ID F1‐score (X% of the time tracker’s IDs match the ground truth over all frames)\n",
    "           'num_switches',  # total ID switches\n",
    "           'mostly_tracked', # MT (# of GT tracks tracked for at least 80% of their lifespan)\n",
    "           'mostly_lost']    # ML (# of GT tracks tracked for less than 20% of their lifespan)\n",
    "summary_id = mh.compute(\n",
    "    acc,\n",
    "    metrics=metrics,\n",
    "    name='ID-Only'\n",
    ")\n",
    "print(mm.io.render_summary(summary_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8874a8eb",
   "metadata": {},
   "source": [
    "## 6. Side-by-Side Frame-by-Frame Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e227f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display each frame with GT boxes on the left and prediction boxes on the right, with a slider to navigate.\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import cv2\n",
    "\n",
    "# Ensure images load correctly\n",
    "\n",
    "# Visualization function\n",
    "def display_frame_side_by_side(frame):\n",
    "    clear_output(wait=True)\n",
    "    # Find image name for this frame\n",
    "    img_name = gt_df.loc[gt_df.frame == frame, 'image_name'].iloc[0]\n",
    "    img_path = os.path.join(DATA_DIR, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Could not load image for frame {frame}: {img_path}\")\n",
    "        return\n",
    "    # Prepare copies\n",
    "    vis_gt = img.copy()\n",
    "    vis_pr = img.copy()\n",
    "    # Draw GT boxes (green)\n",
    "    for _, row in gt_df[gt_df.frame == frame].iterrows():\n",
    "        if row[['x1','y1','x2','y2']].isnull().any():\n",
    "            continue\n",
    "        x1, y1, x2, y2 = map(int, row[['x1','y1','x2','y2']])\n",
    "        cv2.rectangle(vis_gt, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "    # Draw Pred boxes (red) with IDs\n",
    "    font       = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1.5\n",
    "    thickness  = 1\n",
    "    for _, row in pr_df[pr_df.frame == frame].iterrows():\n",
    "        x1, y1, x2, y2 = map(int, row[['x1','y1','x2','y2']])\n",
    "        trk_id = str(row['trk_id'])\n",
    "\n",
    "        # Draw the box\n",
    "        cv2.rectangle(vis_pr, (x1, y1), (x2, y2), (0,0,255), 2)\n",
    "\n",
    "        # Measure text size so we can adjust if it would go outside\n",
    "        (text_w, text_h), baseline = cv2.getTextSize(trk_id, font, font_scale, thickness)\n",
    "\n",
    "        # Try to put text above the box; if not enough space, put it below\n",
    "        text_x = x1\n",
    "        if y1 - text_h - baseline > 0:\n",
    "            text_y = y1 - baseline\n",
    "        else:\n",
    "            text_y = y1 + text_h + baseline\n",
    "\n",
    "        # Clamp horizontally so text doesn’t overflow\n",
    "        text_x = max(0, min(text_x, vis_pr.shape[1] - text_w))\n",
    "\n",
    "        # Finally, draw the text background for readability (optional)\n",
    "        cv2.rectangle(vis_pr,\n",
    "                      (text_x, text_y - text_h - baseline),\n",
    "                      (text_x + text_w, text_y + baseline),\n",
    "                      (255,255,255), \n",
    "                      thickness=cv2.FILLED)\n",
    "        # Draw the ID text in red\n",
    "        cv2.putText(vis_pr, trk_id, (text_x, text_y), font, font_scale, (0,0,255), thickness)\n",
    "    # Plot side by side\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,6))\n",
    "    axes[0].imshow(cv2.cvtColor(vis_gt, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title(f'Frame {frame} GT')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(cv2.cvtColor(vis_pr, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(f'Frame {frame} Pred')\n",
    "    axes[1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Slider to navigate frames\n",
    "total_frames = sorted(gt_df['frame'].unique())\n",
    "frame_slider = widgets.SelectionSlider(\n",
    "    options=total_frames,\n",
    "    description='Frame:',\n",
    "    continuous_update=False\n",
    ")\n",
    "widgets.interact(display_frame_side_by_side, frame=frame_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e137dc38",
   "metadata": {},
   "source": [
    "## 7. Compute Full CLEAR MOT Metrics (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e17abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_full = mm.metrics.create()\n",
    "metrics_full = ['mota', 'motp', 'idf1', 'num_switches', 'mostly_tracked', 'mostly_lost']\n",
    "summary_full = mh_full.compute(\n",
    "    acc,\n",
    "    metrics=metrics_full,\n",
    "    name='Overall'\n",
    ")\n",
    "print(\"Full CLEAR MOT Metrics:\")\n",
    "print(mm.io.render_summary(summary_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918b8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
