{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell tells python to use the local version of CVToolkit instead of the installed one.\n",
    "# Use for testing purposes until branch feature/BCV-970-oor-metrics is merged.\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../../CVToolkit\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.metrics.per_pixel_stats import (\n",
    "    EvaluatePixelWise,\n",
    ")\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.metrics.metrics_utils import (\n",
    "    ObjectClass,\n",
    "    BoxSize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: ground truth and predictions\n",
    "gt_path = \"../../datasets/oor/processed-first-official-training-dataset-oor/labels/val\"\n",
    "pred_path = \"../../experiments/pred_val_yolov8m_1280/labels\"\n",
    "\n",
    "# Size of the images (width, height)\n",
    "img_shape = (3840, 2160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute performance over classes and bounding box sizes\n",
    "\n",
    "evaluator = EvaluatePixelWise(gt_path, pred_path, img_shape)\n",
    "\n",
    "\n",
    "# Compute the Total Blurred Area results for the person & licence plate classes\n",
    "\n",
    "tba_results = evaluator.collect_results_per_class_and_size(\n",
    "    classes=[ObjectClass.person, ObjectClass.license_plate]\n",
    ")\n",
    "evaluator.store_tba_results(results=tba_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per pixel results \n",
    "\n",
    "per_pixel_results = evaluator.collect_results_per_class_and_size(\n",
    "    classes=ObjectClass,\n",
    "    box_sizes=[BoxSize.all]\n",
    ")\n",
    "\n",
    "pd.DataFrame(data=per_pixel_results).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from pycocotools.coco import COCO\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.metrics.custom_coco_evaluator import CustomCOCOeval\n",
    "\n",
    "\n",
    "def coco_evaluation(\n",
    "    coco_annotations_json: str,\n",
    "    coco_predictions_json: str,\n",
    "    metrics_metadata: dict,\n",
    "    class_ids: List[int] = [0, 1, 2, 3, 4]\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs COCO evaluation on the output of YOLO validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coco_annotations_json: annotations in the COCO format compatible with yolov5. Comes from the metadata pipeline\n",
    "    coco_predictions_json: predictions in COCO format of the yolov5 run.\n",
    "    metrics_metadata: info about image sizes and areas for sanity checks.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    COCO_gt = COCO(coco_annotations_json)  # init annotations api\n",
    "    try:\n",
    "        COCO_dt = COCO_gt.loadRes(coco_predictions_json)  # init predictions api\n",
    "    except FileNotFoundError:\n",
    "        raise Exception(\n",
    "            f\"The specified file '{coco_predictions_json}' was not found.\"\n",
    "            f\"The file is created at the above path if you run yolo validation with\"\n",
    "            f\"the --save-json flag enabled.\"\n",
    "        )\n",
    "    evaluation = CustomCOCOeval(COCO_gt, COCO_dt, \"bbox\")\n",
    "\n",
    "    # Opening JSON file\n",
    "    with open(coco_annotations_json) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    height = data[\"images\"][0][\"height\"]\n",
    "    width = data[\"images\"][0][\"width\"]\n",
    "    if (\n",
    "        height != metrics_metadata[\"image_height\"]\n",
    "        or width != metrics_metadata[\"image_width\"]\n",
    "    ):\n",
    "        print(\n",
    "            f\"You're trying to run evaluation on images of size {height} x {width}, \"\n",
    "            \"but the coco annotations have been generated from images of size \"\n",
    "            f\"{metrics_metadata['image_height']} x {metrics_metadata['image_width']}.\"\n",
    "            \"Why is it a problem? Because the coco annotations that the metadata produces and the \"\n",
    "            \" *_predictions.json produced by the yolo run are both in absolute format,\"\n",
    "            \"so we must compare use the same image sizes.\"\n",
    "            \"Solutions: 1. Use images for validation that are the same size as the ones you used in the \"\n",
    "            \"data labeling project. 2. After you export the json from the labeling project, overwrite\"\n",
    "            \"the heights and widths from the images, since that version is still using normalized values.\"\n",
    "        )\n",
    "\n",
    "    image_names = [image[\"id\"] for image in data[\"images\"]]\n",
    "    evaluation.params.imgIds = image_names  # image IDs to evaluate\n",
    "    evaluation.params.catIds = class_ids\n",
    "\n",
    "    img_area = metrics_metadata[\"image_width\"] * metrics_metadata[\"image_height\"]\n",
    "\n",
    "    areaRng = []\n",
    "    for areaRngLbl in evaluation.params.areaRngLbl:\n",
    "        aRng = {\"areaRngLbl\": areaRngLbl}\n",
    "        for obj_cls in ObjectClass:\n",
    "            box = BoxSize.from_objectclass(obj_cls).__getattribute__(areaRngLbl)\n",
    "            aRng[obj_cls.value] = (box[0]*img_area, box[1]*img_area)\n",
    "        areaRng.append(aRng)\n",
    "\n",
    "    evaluation.params.areaRng = areaRng\n",
    "\n",
    "    evaluation.evaluate()\n",
    "    evaluation.accumulate()\n",
    "    evaluation.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_metadata = {\"image_width\": 3840, \"image_height\": 2160}\n",
    "\n",
    "coco_annotations = \"../../datasets/oor/processed-first-official-training-dataset-oor/coco_annotations_val.json\"\n",
    "coco_predictions = \"../../experiments/val_val_yolov8m_1280/predictions.json\"\n",
    "\n",
    "coco_evaluation(\n",
    "    coco_annotations_json=coco_annotations,\n",
    "    coco_predictions_json=coco_predictions,\n",
    "    metrics_metadata=metrics_metadata,\n",
    "    class_ids=[2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
