{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell tells python to use the local version of CVToolkit instead of the installed one.\n",
    "# Use for testing purposes until branch feature/BCV-970-oor-metrics is merged.\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../../CVToolkit\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.oor_evaluation import (\n",
    "    tba_evaluation, coco_evaluation\n",
    ")\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.metrics.metrics_utils import (\n",
    "    BoxSize, ObjectClass, predictions_to_coco_json\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def store_tba_results(\n",
    "    results: Dict[str, Dict],\n",
    "    markdown_output_path: str = \"tba_results.md\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Store information from the results dict into a markdown file.\n",
    "    In this case, the recall from the Total Blurred Area is the only interest number.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results: dictionary with results\n",
    "    markdown_output_path\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    with open(markdown_output_path, \"w\") as f:\n",
    "        f.write(\n",
    "            \" Model | Person Small | Person Medium | Person Large | Person ALL |\"\n",
    "            \" License Plate Small |  License Plate Medium  | License Plate Large | Licence Plate ALL |\\n\"\n",
    "        )\n",
    "        f.write(\n",
    "            \"| ----- | ----- | ----- |  ----- | ----- | ----- | ----- | ----- | ----- |\\n\"\n",
    "        )\n",
    "        for model, rslt in results.items():\n",
    "            f.write(\n",
    "                f'| {model} | {rslt[\"person_small\"][\"recall\"]} | {rslt[\"person_medium\"][\"recall\"]} '\n",
    "                f'| {rslt[\"person_large\"][\"recall\"]} | {rslt[\"person_all\"][\"recall\"]} '\n",
    "                f'| {rslt[\"license_plate_small\"][\"recall\"]} | {rslt[\"license_plate_medium\"][\"recall\"]} '\n",
    "                f'| {rslt[\"license_plate_large\"][\"recall\"]} | {rslt[\"license_plate_all\"][\"recall\"]}\\n'\n",
    "            )\n",
    "        f.write(\n",
    "            f\"\\nThresholds used for these calculations: Person=`{BoxSize.from_objectclass(ObjectClass.person)}`, \"\n",
    "            f\"License Plate=`{BoxSize.from_objectclass(ObjectClass.license_plate)}`.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: ground truth and predictions\n",
    "gt_annotations_folder = \"../../datasets/oor/processed-first-official-training-dataset-oor/labels/val\"\n",
    "gt_coco_json = \"../../datasets/oor/processed-first-official-training-dataset-oor/coco_annotations_val.json\"\n",
    "\n",
    "# Size of the images (width, height)\n",
    "img_shape = (3840, 2160)\n",
    "\n",
    "# models = [\"yolov8s_1024\", \"yolov8m_1024\", \"yolov8s_1280\", \"yolov8m_1280\", \"yolov8m_1920\"]\n",
    "models = [\"yolov8m_1024\", \"yolov8m_1920\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TBA results\n",
    "\n",
    "tba_results = dict()\n",
    "\n",
    "for model in models:\n",
    "    pred_folder = f\"../../experiments/pred_val_{model}/labels\"\n",
    "    tba_results_file = f\"../../experiments/pred_val_{model}/tba_results.md\"\n",
    "\n",
    "    tba_results[model] = tba_evaluation(\n",
    "        ground_truth_folder=gt_annotations_folder,\n",
    "        prediction_folder=pred_folder,\n",
    "        image_shape=img_shape,\n",
    "        save_results=True,\n",
    "        results_file=tba_results_file,\n",
    "        hide_progress=True,\n",
    "        upper_half=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_tba_results(tba_results, markdown_output_path=\"tba_results.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per pixel results \n",
    "\n",
    "if False:\n",
    "    per_pixel_results = tba_evaluation(\n",
    "        ground_truth_folder=gt_annotations_folder,\n",
    "        prediction_folder=pred_folder,\n",
    "        image_shape=img_shape,\n",
    "        object_classes=ObjectClass,\n",
    "        single_size_only=True,\n",
    "        save_results=False,\n",
    "    )\n",
    "\n",
    "    pd.DataFrame(data=per_pixel_results).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute COCO evaluation\n",
    "\n",
    "data_labels = [\n",
    "    \"model_size\",\n",
    "    \"img_size\",\n",
    "    \"target_class\",\n",
    "    \"AP@50-95_all\",\n",
    "    \"AP@75_all\",\n",
    "    \"AP@50_all\",\n",
    "    \"AP@50_small\",\n",
    "    \"AP@50_medium\",\n",
    "    \"AP@50_large\",\n",
    "    \"AR@50-95_all\",\n",
    "    \"AR@75_all\",\n",
    "    \"AR@50_all\",\n",
    "    \"AR@50_small\",\n",
    "    \"AR@50_medium\",\n",
    "    \"AR@50_large\",\n",
    "]\n",
    "\n",
    "target_classes = [[0, 1, 2], [0], [1], [2]]\n",
    "target_class_names = [\"all\", \"person\", \"license_plate\", \"container\"]\n",
    "\n",
    "coco_df = pd.DataFrame(columns=data_labels)\n",
    "\n",
    "for model in models:\n",
    "    pred_folder = f\"../../experiments/pred_val_{model}/labels\"\n",
    "    pred_coco_json = f\"../../experiments/pred_val_{model}/coco_predictions.json\"\n",
    "\n",
    "    # if not os.path.isfile(pred_coco_json):\n",
    "    predictions_to_coco_json(predictions_folder=pred_folder, image_shape=img_shape, json_file=pred_coco_json)\n",
    "\n",
    "    model_size = model.split(sep=\"_\")[0][-1]\n",
    "    img_size = int(model.split(sep=\"_\")[1])\n",
    "\n",
    "    for target_cls_name, target_cls in zip(target_class_names, target_classes):\n",
    "        print(f\"EVALUATING {model}, TARGET CLASS {target_cls_name}\")\n",
    "        eval = coco_evaluation(\n",
    "            coco_annotations_json=gt_coco_json,\n",
    "            coco_predictions_json=pred_coco_json,\n",
    "            predicted_img_shape=img_shape,\n",
    "            class_ids=target_cls,\n",
    "            print_summary=True,\n",
    "        )\n",
    "        coco_df.loc[f\"{model}_{target_cls_name}\"] = [model_size, img_size, target_cls_name, *eval.stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_df.to_csv(\"coco_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_show = [\n",
    "    \"target_class\",\n",
    "    \"AP@50_all\",\n",
    "    \"AP@50_small\",\n",
    "    \"AP@50_medium\",\n",
    "    \"AP@50_large\",\n",
    "    \"AR@50_all\",\n",
    "    \"AR@50_small\",\n",
    "    \"AR@50_medium\",\n",
    "    \"AR@50_large\",\n",
    "]\n",
    "\n",
    "demo_df = coco_df[(coco_df[\"model_size\"]==\"m\") & (coco_df[\"img_size\"].isin((1024, 1920)))]\n",
    "demo_df = demo_df[cols_to_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df[demo_df[\"target_class\"]==\"all\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df[demo_df[\"target_class\"]==\"container\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
