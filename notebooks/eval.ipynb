{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell tells python to use the local version of CVToolkit instead of the installed one.\n",
    "# Use for testing purposes until branch feature/BCV-970-oor-metrics is merged.\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../../CVToolkit\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.oor_evaluation import (\n",
    "    tba_evaluation, coco_evaluation\n",
    ")\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.metrics.metrics_utils import (\n",
    "    BoxSize, ObjectClass, predictions_to_coco_json\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: ground truth and predictions\n",
    "gt_annotations_folder = \"../../datasets/oor/processed-first-official-training-dataset-oor/labels/val\"\n",
    "gt_coco_json = \"../../datasets/oor/processed-first-official-training-dataset-oor/coco_annotations_val.json\"\n",
    "\n",
    "# Size of the images (width, height)\n",
    "img_shape = (3840, 2160)\n",
    "\n",
    "# models = [\"yolov8s_1024\", \"yolov8m_1024\", \"yolov8s_1280\", \"yolov8m_1280\", \"yolov8m_1920\"]\n",
    "models = [\"yolov8m_1280\", \"yolov8m_1920\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvtoolkit.datasets.yolo_labels_dataset import YoloLabelsDataset\n",
    "\n",
    "ds1 = YoloLabelsDataset(\n",
    "    folder_path=gt_annotations_folder,\n",
    "    image_area=img_shape[0]*img_shape[1],\n",
    ")\n",
    "ds2 = YoloLabelsDataset.from_yolo_validation_json(\n",
    "    yolo_val_json=gt_coco_json,\n",
    "    image_shape=img_shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TBA results\n",
    "\n",
    "tba_results = []\n",
    "\n",
    "for model in models:\n",
    "    pred_folder = f\"../../experiments/pred_val_{model}/labels\"\n",
    "    # pred_folder = f\"../../experiments/pred_val_{model}/coco_predictions.json\"\n",
    "    tba_results_file = f\"../../experiments/pred_val_{model}/tba_results.md\"\n",
    "\n",
    "    tba_results.append(\n",
    "        tba_evaluation(\n",
    "            ground_truth_folder=gt_annotations_folder,\n",
    "            # ground_truth_folder=gt_coco_json,\n",
    "            prediction_folder=pred_folder,\n",
    "            image_shape=img_shape,\n",
    "            save_results=False,\n",
    "            results_file=tba_results_file,\n",
    "            hide_progress=True,\n",
    "            upper_half=False,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.metrics.per_pixel_stats import EvaluatePixelWise\n",
    "\n",
    "EvaluatePixelWise.store_tba_results(tba_results, model_name=models, markdown_output_path=\"tba_results_old.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per pixel results \n",
    "\n",
    "if False:\n",
    "    per_pixel_results = tba_evaluation(\n",
    "        ground_truth_folder=gt_annotations_folder,\n",
    "        prediction_folder=pred_folder,\n",
    "        image_shape=img_shape,\n",
    "        object_classes=ObjectClass,\n",
    "        single_size_only=True,\n",
    "        save_results=False,\n",
    "    )\n",
    "\n",
    "    pd.DataFrame(data=per_pixel_results).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute COCO evaluation\n",
    "\n",
    "data_labels = [\n",
    "    \"model_size\",\n",
    "    \"img_size\",\n",
    "    \"target_class\",\n",
    "    \"AP@50-95_all\",\n",
    "    \"AP@75_all\",\n",
    "    \"AP@50_all\",\n",
    "    \"AP@50_small\",\n",
    "    \"AP@50_medium\",\n",
    "    \"AP@50_large\",\n",
    "    \"AR@50-95_all\",\n",
    "    \"AR@75_all\",\n",
    "    \"AR@50_all\",\n",
    "    \"AR@50_small\",\n",
    "    \"AR@50_medium\",\n",
    "    \"AR@50_large\",\n",
    "]\n",
    "\n",
    "target_classes = [[0, 1, 2], [0], [1], [2]]\n",
    "target_class_names = [\"all\", \"person\", \"license_plate\", \"container\"]\n",
    "\n",
    "coco_df = pd.DataFrame(columns=data_labels)\n",
    "\n",
    "for model in models:\n",
    "    pred_folder = f\"../../experiments/pred_val_{model}/labels\"\n",
    "    pred_coco_json = f\"../../experiments/pred_val_{model}/coco_predictions.json\"\n",
    "\n",
    "    # if not os.path.isfile(pred_coco_json):\n",
    "    predictions_to_coco_json(predictions_folder=pred_folder, image_shape=img_shape, json_file=pred_coco_json)\n",
    "\n",
    "    model_size = model.split(sep=\"_\")[0][-1]\n",
    "    img_size = int(model.split(sep=\"_\")[1])\n",
    "\n",
    "    for target_cls_name, target_cls in zip(target_class_names, target_classes):\n",
    "        print(f\"EVALUATING {model}, TARGET CLASS {target_cls_name}\")\n",
    "        eval = coco_evaluation(\n",
    "            coco_annotations_json=gt_coco_json,\n",
    "            coco_predictions_json=pred_coco_json,\n",
    "            predicted_img_shape=img_shape,\n",
    "            class_ids=target_cls,\n",
    "            print_summary=True,\n",
    "        )\n",
    "        coco_df.loc[f\"{model}_{target_cls_name}\"] = [model_size, img_size, target_cls_name, *eval.stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_df.to_csv(\"coco_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "cols_to_show = [\n",
    "    \"target_class\",\n",
    "    \"AP@50_all\",\n",
    "    \"AP@50_small\",\n",
    "    \"AP@50_medium\",\n",
    "    \"AP@50_large\",\n",
    "    \"AR@50_all\",\n",
    "    \"AR@50_small\",\n",
    "    \"AR@50_medium\",\n",
    "    \"AR@50_large\",\n",
    "]\n",
    "\n",
    "demo_df = coco_df[(coco_df[\"model_size\"]==\"m\") & (coco_df[\"img_size\"].isin((1024, 1920)))]\n",
    "demo_df = demo_df[cols_to_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df[demo_df[\"target_class\"]==\"all\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df[demo_df[\"target_class\"]==\"container\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
