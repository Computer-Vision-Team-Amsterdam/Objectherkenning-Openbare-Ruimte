{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell tells python to use the local version of CVToolkit instead of the installed one.\n",
    "# Use for testing purposes until branch feature/BCV-970-oor-metrics is merged.\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../../CVToolkit\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.source.oor_evaluation import (\n",
    "    tba_evaluation, per_image_evaluation, coco_evaluation\n",
    ")\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.metrics.metrics_utils import (\n",
    "    ObjectClass, predictions_to_coco_json\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the images (width, height)\n",
    "img_shape = (1280, 720)\n",
    "gt_base_dir = \"../../datasets/oor/processed-merged-batches-first-official-training-dataset-oor\"\n",
    "pred_base_dir = \"../../datasets/oor/inference/processed_merged_v2\"\n",
    "\n",
    "models = [\"yolov8m_1280_oor_v2_best\", \"yolov8_1280_oor_v2_noble_sweep_15\", \"yolov8_1280_oor_v2_cerulean_sweep_25\"]\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed look into one specific model / run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = splits[2]\n",
    "model = models[1]\n",
    "\n",
    "gt_annotations_folder = f\"{gt_base_dir}/labels/{split}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load frame metadata\n",
    "\n",
    "import pathlib\n",
    "import geopandas as gpd\n",
    "from typing import List, Union\n",
    "\n",
    "from cvtoolkit.datasets.yolo_labels_dataset import YoloLabelsDataset\n",
    "\n",
    "metadata_folder = \"../../datasets/oor/metadata\"\n",
    "\n",
    "RD_CRS = \"EPSG:28992\"  # CRS code for the Dutch Rijksdriehoek coordinate system\n",
    "LAT_LON_CRS = \"EPSG:4326\"  # CRS code for WGS84 latitude/longitude coordinate system\n",
    "\n",
    "def metadata_to_video_name(metadata_name: str) -> str:\n",
    "    metadata_split = metadata_name.split(sep=\"-\", maxsplit=1)\n",
    "    return f\"{metadata_split[0]}-0-{metadata_split[1]}\"\n",
    "\n",
    "def load_metadata_csv(metadata_file: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(metadata_file)\n",
    "    video_name = metadata_to_video_name(pathlib.Path(metadata_file).stem)\n",
    "    df[\"frame_name\"] = [f\"{video_name}_{frame_id:04}\" for frame_id in df[\"new_frame_id\"]]\n",
    "    return df.set_index(\"frame_name\")\n",
    "\n",
    "def get_target_cls_file_names(yolo_annotations_folder: str, target_cls: Union[ObjectClass, None] = None) -> List[str]:\n",
    "    yolo_dataset = YoloLabelsDataset(\n",
    "        folder_path=yolo_annotations_folder,\n",
    "        image_area=img_shape[0]*img_shape[1],\n",
    "    )\n",
    "    if target_cls:\n",
    "        yolo_dataset.filter_by_class(target_cls.value)\n",
    "    target_labels = yolo_dataset._filtered_labels\n",
    "    return [k for k, v in target_labels.items() if len(v) > 0]\n",
    "\n",
    "metadata_files = pathlib.Path(metadata_folder).glob(\"*.csv\")\n",
    "metadata_df = pd.concat(\n",
    "    [load_metadata_csv(metadata_file) for metadata_file in metadata_files]\n",
    ")\n",
    "\n",
    "metadata_gdf = gpd.GeoDataFrame(\n",
    "    metadata_df,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        x=metadata_df.gps_lon,\n",
    "        y=metadata_df.gps_lat,\n",
    "        crs=LAT_LON_CRS,\n",
    "    ),\n",
    ").to_crs(RD_CRS)\n",
    "\n",
    "del metadata_df, metadata_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all detections of containers\n",
    "\n",
    "# Ground truth\n",
    "gt_container_names = get_target_cls_file_names(gt_annotations_folder, ObjectClass.container)\n",
    "keep_index = [frame in gt_container_names for frame in metadata_gdf.index]\n",
    "gt_gdf = metadata_gdf[keep_index]\n",
    "gt_gdf = gt_gdf[[\"gps_state\", \"geometry\"]]\n",
    "\n",
    "# Predictions\n",
    "pred_folder = f\"{pred_base_dir}/{model}/labels/{split}\"\n",
    "\n",
    "pred_container_names = get_target_cls_file_names(pred_folder, ObjectClass.container)\n",
    "keep_index = [frame in pred_container_names for frame in metadata_gdf.index]\n",
    "pred_gdf = metadata_gdf[keep_index]\n",
    "pred_gdf = pred_gdf[[\"gps_state\", \"geometry\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distances between ground truth and detections\n",
    "gt_gdf[\"distance\"] = gt_gdf[\"geometry\"].distance(pred_gdf[\"geometry\"].unary_union)\n",
    "pred_gdf[\"distance\"] = pred_gdf[\"geometry\"].distance(gt_gdf[\"geometry\"].unary_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distance statistics\n",
    "import numpy as np\n",
    "\n",
    "stats = {\n",
    "    \"distance\": np.arange(0, 26, 5),\n",
    "    \"fnr\": [],\n",
    "    \"fpr\": [],\n",
    "}\n",
    "\n",
    "gt_total = len(gt_gdf)\n",
    "pred_total = len(pred_gdf)\n",
    "\n",
    "for dst in stats[\"distance\"]:\n",
    "    fn = np.count_nonzero(gt_gdf[\"distance\"] > dst)\n",
    "    fp = np.count_nonzero(pred_gdf[\"distance\"] > dst)\n",
    "    stats[\"fnr\"].append(fn/gt_total)\n",
    "    stats[\"fpr\"].append(fp/pred_total)\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results on a map\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "joined_gdf = gt_gdf.join(pred_gdf, how=\"outer\", lsuffix=\"_gt\", rsuffix=\"_pred\")\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "joined_gdf.set_geometry(\"geometry_gt\").plot(ax=ax, markersize=20)\n",
    "joined_gdf.set_geometry(\"geometry_pred\").plot(ax=ax, color=\"red\", markersize=5)\n",
    "\n",
    "plt.savefig(\"val_map.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Total Blurred Area for sensitive object classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TBA results\n",
    "\n",
    "tba_results = []\n",
    "tba_names = []\n",
    "\n",
    "for model in models:\n",
    "    for split in splits:\n",
    "        print(f\"Computing TBA for {model}/{split}...\")\n",
    "        tba_names.append(f\"{model}_{split}\")\n",
    "\n",
    "        gt_annotations_folder = f\"{gt_base_dir}/labels/{split}\"\n",
    "        pred_folder = f\"{pred_base_dir}/{model}/labels/{split}\"\n",
    "        tba_results_file = f\"{pred_base_dir}/{model}/tba_results_{split}.md\"\n",
    "\n",
    "        tba_results.append(\n",
    "            tba_evaluation(\n",
    "                ground_truth_folder=gt_annotations_folder,\n",
    "                prediction_folder=pred_folder,\n",
    "                image_shape=img_shape,\n",
    "                save_results=True,\n",
    "                results_file=tba_results_file,\n",
    "                hide_progress=True,\n",
    "                upper_half=False,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.metrics.per_pixel_stats import EvaluatePixelWise\n",
    "\n",
    "EvaluatePixelWise.store_tba_results(tba_results, model_name=tba_names, markdown_output_path=\"tba_results.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = [\n",
    "    \"split\",\n",
    "    \"target_class\",\n",
    "    \"Precision\",\n",
    "    \"Recall\",\n",
    "    \"FPR\",\n",
    "    \"FNR\",\n",
    "    \"TNR\",\n",
    "]\n",
    "\n",
    "target_classes = [ObjectClass.container, ObjectClass.mobile_toilet, ObjectClass.scaffolding]\n",
    "\n",
    "img_stat_df = pd.DataFrame(columns=data_labels)\n",
    "\n",
    "names = []\n",
    "results = []\n",
    "\n",
    "for model in models:\n",
    "    for split in splits:\n",
    "        gt_annotations_folder = f\"{gt_base_dir}/labels/{split}\"\n",
    "        pred_annotations_folder = f\"{pred_base_dir}/{model}/labels/{split}\"\n",
    "\n",
    "        names.append(f\"{model}_{split}\")\n",
    "        results.append(per_image_evaluation(\n",
    "            ground_truth_folder=gt_annotations_folder,\n",
    "            prediction_folder=pred_annotations_folder,\n",
    "            image_shape=img_shape,\n",
    "            object_classes=target_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create df out of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_stat_df[img_stat_df[\"target_class\"] == \"container\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run custom COCO evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute COCO evaluation\n",
    "\n",
    "data_labels = [\n",
    "    \"model_size\",\n",
    "    \"img_size\",\n",
    "    \"split\",\n",
    "    \"target_class\",\n",
    "    \"AP@50-95_all\",\n",
    "    \"AP@75_all\",\n",
    "    \"AP@50_all\",\n",
    "    \"AP@50_small\",\n",
    "    \"AP@50_medium\",\n",
    "    \"AP@50_large\",\n",
    "    \"AR@50-95_all\",\n",
    "    \"AR@75_all\",\n",
    "    \"AR@50_all\",\n",
    "    \"AR@50_small\",\n",
    "    \"AR@50_medium\",\n",
    "    \"AR@50_large\",\n",
    "]\n",
    "\n",
    "target_classes = [[0, 1, 2], [0], [1], [2], [3], [4]]\n",
    "target_class_names = [\"all\", \"person\", \"license_plate\", \"container\", \"mobile_toilet\", \"scaffolding\"]\n",
    "\n",
    "coco_df = pd.DataFrame(columns=data_labels)\n",
    "\n",
    "for model in models:\n",
    "    for split in splits:\n",
    "        pred_folder = f\"{pred_base_dir}/{model}/labels/{split}\"\n",
    "        pred_coco_json = f\"{pred_base_dir}/{model}/coco_predictions_{split}.json\"\n",
    "        gt_coco_json = f\"{gt_base_dir}/coco_gt_{split}.json\"\n",
    "\n",
    "        # if not os.path.isfile(pred_coco_json):\n",
    "        predictions_to_coco_json(predictions_folder=pred_folder, image_shape=img_shape, json_file=pred_coco_json)\n",
    "\n",
    "        model_size = model.split(sep=\"_\")[0][-1]\n",
    "        img_size = int(model.split(sep=\"_\")[1])\n",
    "\n",
    "        for target_cls_name, target_cls in zip(target_class_names, target_classes):\n",
    "            print(f\"EVALUATING {model} / {split}, TARGET CLASS {target_cls_name}\")\n",
    "            eval = coco_evaluation(\n",
    "                coco_annotations_json=gt_coco_json,\n",
    "                coco_predictions_json=pred_coco_json,\n",
    "                predicted_img_shape=img_shape,\n",
    "                class_ids=target_cls,\n",
    "                print_summary=True,\n",
    "            )\n",
    "            coco_df.loc[f\"{model}_{split}_{target_cls_name}\"] = [model_size, img_size, split, target_cls_name, *eval.stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_df.to_csv(\"coco_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "cols_to_show = [\n",
    "    \"target_class\",\n",
    "    \"AP@50-95_all\",\n",
    "    \"AP@50_all\",\n",
    "    \"AP@50_small\",\n",
    "    \"AP@50_medium\",\n",
    "    \"AP@50_large\",\n",
    "    \"AR@50-95_all\",\n",
    "    \"AR@50_all\",\n",
    "    \"AR@50_small\",\n",
    "    \"AR@50_medium\",\n",
    "    \"AR@50_large\",\n",
    "]\n",
    "\n",
    "# demo_df = coco_df[(coco_df[\"model_size\"]==\"m\") & (coco_df[\"img_size\"].isin((1024, 1920)))]\n",
    "demo_df = coco_df\n",
    "demo_df = demo_df[cols_to_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df[demo_df[\"target_class\"]==\"all\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df[demo_df[\"target_class\"]==\"container\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df[demo_df[\"target_class\"]==\"person\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_df[coco_df[\"target_class\"]==\"container\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
