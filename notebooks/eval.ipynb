{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell tells python to use the local version of CVToolkit instead of the installed one.\n",
    "# Use for testing purposes until branch feature/BCV-970-oor-metrics is merged.\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../../CVToolkit\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.metrics.per_pixel_stats import (\n",
    "    EvaluatePixelWise,\n",
    ")\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.metrics.metrics_utils import (\n",
    "    ObjectClass,\n",
    "    BoxSize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: ground truth and predictions\n",
    "gt_path = \"../../datasets/oor/eval_metrics_test/labels/ground_truth\"\n",
    "pred_path = \"../../datasets/oor/eval_metrics_test/labels/predictions\"\n",
    "\n",
    "# Size of the images (width, height)\n",
    "img_shape = (1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute performance over classes and bounding box sizes\n",
    "\n",
    "evaluator = EvaluatePixelWise(gt_path, pred_path, img_shape)\n",
    "\n",
    "\n",
    "# Compute the Total Blurred Area results for the person & licence plate classes\n",
    "\n",
    "tba_results = evaluator.collect_results_per_class_and_size(\n",
    "    classes=[ObjectClass.person, ObjectClass.license_plate],\n",
    "    box_sizes=BoxSize\n",
    ")\n",
    "evaluator.store_tba_results(results=tba_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per pixel results \n",
    "\n",
    "per_pixel_results = evaluator.collect_results_per_class_and_size(\n",
    "    classes=ObjectClass,\n",
    "    box_sizes=[BoxSize.all]\n",
    ")\n",
    "\n",
    "pd.DataFrame(data=per_pixel_results).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Tuple, Union\n",
    "from torch import tensor, empty\n",
    "\n",
    "def cxcywh_norm_to_xywh(cxcywh: Tuple[float, float, float, float], image_shape: Tuple[int, int]) -> Tuple[float, float, float, float]:\n",
    "    (img_width, img_height) = image_shape\n",
    "    x_center, y_center, width, height = cxcywh\n",
    "    x_center, y_center, width, height = (\n",
    "        x_center * img_width,\n",
    "        y_center * img_height,\n",
    "        width * img_width,\n",
    "        height * img_height,\n",
    "    )\n",
    "    x_min = (x_center - width / 2)\n",
    "    y_min = (y_center - height / 2)\n",
    "    return x_min, y_min, width, height\n",
    "\n",
    "def yolo_to_cl_xywh(yolo_annotation: str, image_shape: Tuple[int, int]) -> Tuple[int, Tuple[float, float, float, float]]:\n",
    "    cls, cx, cy, w, h = map(\n",
    "        float, yolo_annotation.split()\n",
    "    )\n",
    "    return int(cls), cxcywh_norm_to_xywh((cx, cy, w, h), image_shape)\n",
    "\n",
    "def yolo_pred_to_cl_xywh_conf(yolo_annotation: str, image_shape: Tuple[int, int]) -> Tuple[int, Tuple[float, float, float, float], float]:\n",
    "    # cls, cx, cy, w, h, conf = map(\n",
    "    #     float, yolo_annotation.split()\n",
    "    # )\n",
    "    cls, cx, cy, w, h = map(\n",
    "        float, yolo_annotation.split()\n",
    "    )\n",
    "    conf = 0.536\n",
    "    return int(cls), cxcywh_norm_to_xywh((cx, cy, w, h), image_shape), conf\n",
    "\n",
    "def yolo_annotation_file_to_torchmetrics_dict(yolo_annotation_file: Union[str, os.PathLike], image_shape: Tuple[int, int], is_pred: bool = False) -> Dict[str, Any]:\n",
    "    with open(yolo_annotation_file, mode='r') as f:\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for line in f.readlines():\n",
    "            if is_pred:\n",
    "                cls, box, conf = yolo_pred_to_cl_xywh_conf(line, image_shape)\n",
    "                boxes.append(box)\n",
    "                labels.append(cls)\n",
    "                scores.append(conf)\n",
    "            else:\n",
    "                cls, box = yolo_to_cl_xywh(line, image_shape)\n",
    "                boxes.append(box)\n",
    "                labels.append(cls)\n",
    "        torch_dict = {\n",
    "            'boxes': tensor(boxes),\n",
    "            'labels': tensor(labels),\n",
    "        }\n",
    "        if is_pred:\n",
    "            torch_dict['scores'] = tensor(scores)\n",
    "        return torch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pathlib\n",
    "\n",
    "gt_annotations = pathlib.Path(gt_path).glob(\"*.txt\")\n",
    "\n",
    "gt_data = []\n",
    "pred_data = []\n",
    "\n",
    "for gt_file in gt_annotations:\n",
    "    if gt_file.stem.startswith(\"._\"):\n",
    "        continue\n",
    "\n",
    "    gt_dict = yolo_annotation_file_to_torchmetrics_dict(gt_file, image_shape=img_shape)\n",
    "    gt_dict['name'] = gt_file.stem\n",
    "    gt_data.append(gt_dict)\n",
    "\n",
    "    pred_file = os.path.join(pred_path, gt_file.name)\n",
    "    if os.path.isfile(pred_file):\n",
    "        pred_dict = yolo_annotation_file_to_torchmetrics_dict(pred_file, image_shape=img_shape, is_pred=True)\n",
    "    else:\n",
    "        pred_dict = {\n",
    "            'boxes': empty((0, 4)),\n",
    "            'labels': empty((0)),\n",
    "            'scores': empty((0)),\n",
    "        }\n",
    "    pred_dict['name'] = gt_file.stem\n",
    "    pred_data.append(pred_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "ind = 2\n",
    "\n",
    "pprint(gt_data[ind])\n",
    "pprint(pred_data[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "metric = MeanAveragePrecision(box_format=\"xywh\", iou_type=\"bbox\", class_metrics=True)\n",
    "metric.update(pred_data, gt_data)\n",
    "\n",
    "pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
