{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### App Evaluation Results Analysis\n",
    "\n",
    "This notebook contains the code for evaluating app results, including preprocessing, metrics calculation, and visualization.\n",
    "\n",
    "#### Step 1: Preprocess Data\n",
    "\n",
    "This step involves:\n",
    "1. Converting detection files to COCO format\n",
    "2. Aligning IDs between detection and annotation COCO files\n",
    "\n",
    "#### Step 2: Calculating metrics\n",
    "\n",
    "#### Step 3: Plotting results\n",
    "\n",
    "#### Step 4: Creating HTML report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import base64\n",
    "import cv2\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Set, Optional, Any\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output paths\n",
    "detections_dir = \"data/input/raw_detections\"                        # This folder should contain the json's and images of detections\n",
    "annotations_file = \"data/input/raw_annotations/annotations.json\"    # This file is the raw azure coco annotations file.\n",
    "processed_detections_file = \"data/output/processed_detections.json\"\n",
    "processed_annotations_file = \"data/output/processed_annotations.json\"\n",
    "\n",
    "# Constants\n",
    "DEFAULT_CATEGORY_MAPPING = {\n",
    "    0: 1,  # 0 becomes person (1)\n",
    "    1: 2,  # 1 becomes license plate (2)\n",
    "    2: 3,  # 2 becomes container (3)\n",
    "    3: 4,  # 3 becomes mobile toilet (4)\n",
    "    4: 5   # 4 becomes scaffolding (5)\n",
    "}\n",
    "DEFAULT_CATEGORIES = [\n",
    "    {\"id\": 1, \"name\": \"person\"},\n",
    "    {\"id\": 2, \"name\": \"license plate\"},\n",
    "    {\"id\": 3, \"name\": \"container\"},\n",
    "    {\"id\": 4, \"name\": \"mobile toilet\"},\n",
    "    {\"id\": 5, \"name\": \"scaffolding\"}\n",
    "]\n",
    "\n",
    "DEFAULT_WIDTH = 1280\n",
    "DEFAULT_HEIGHT = 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess helpers\n",
    "\n",
    "def create_image_entry(image_id: int, width: int, height: int, \n",
    "                       detection_data: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Create a COCO-formatted image entry from detection metadata.\n",
    "\n",
    "    Args:\n",
    "        image_id (int): Unique identifier for the image.\n",
    "        width (int): Width of the image in pixels.\n",
    "        height (int): Height of the image in pixels.\n",
    "        detection_data (Dict): Dictionary containing metadata with:\n",
    "            - image_file_name (str): Name of the image file.\n",
    "            - image_file_timestamp (str): Timestamp when the image was captured.\n",
    "            - gps_data (Dict): GPS information with keys:\n",
    "                * latitude (float)\n",
    "                * longitude (float)\n",
    "                * accuracy (float)\n",
    "                * coordinate_time_stamp (str)\n",
    "            - record_timestamp (str): Timestamp when the detection was recorded.\n",
    "\n",
    "    Returns:\n",
    "        Dict: A dictionary representing a COCO image entry with fields:\n",
    "            'id', 'width', 'height', 'file_name', 'date_captured',\n",
    "            'gps_data', and 'record_timestamp'.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"id\": image_id,\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "        \"file_name\": detection_data['image_file_name'],\n",
    "        \"date_captured\": detection_data['image_file_timestamp'],\n",
    "        \"gps_data\": {\n",
    "            \"latitude\": detection_data['gps_data']['latitude'],\n",
    "            \"longitude\": detection_data['gps_data']['longitude'],\n",
    "            \"accuracy\": detection_data['gps_data']['accuracy'],\n",
    "            \"coordinate_time_stamp\": detection_data['gps_data']['coordinate_time_stamp']\n",
    "        },\n",
    "        \"record_timestamp\": detection_data['record_timestamp']\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_prediction_box(box: Dict) -> List[float]:\n",
    "    \"\"\"\n",
    "    Convert a predicted bounding box to COCO [x, y, width, height] format.\n",
    "\n",
    "    Args:\n",
    "        box (Dict): Prediction dictionary containing a 'boundingBox' key with:\n",
    "            - x_center (float): Normalized x-coordinate of box center.\n",
    "            - y_center (float): Normalized y-coordinate of box center.\n",
    "            - width (float): Normalized width of the box.\n",
    "            - height (float): Normalized height of the box.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: COCO-formatted box [x, y, width, height], where (x, y)\n",
    "                     corresponds to the top-left corner in normalized coordinates.\n",
    "    \"\"\"\n",
    "    x_center = box['boundingBox']['x_center']\n",
    "    y_center = box['boundingBox']['y_center']\n",
    "    width = box['boundingBox']['width']\n",
    "    height = box['boundingBox']['height']\n",
    "    \n",
    "    # y is inverted\n",
    "    x = x_center\n",
    "    y = 1 - (y_center + height)\n",
    "    \n",
    "    return [x, y, width, height]\n",
    "\n",
    "\n",
    "def group_annotations_by_image(annotations: Dict) -> Dict[int, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Organize a flat list of annotations into groups keyed by image ID.\n",
    "\n",
    "    Args:\n",
    "        annotations (Dict): Dictionary with an 'annotations' key containing\n",
    "                            a list of annotation dicts, each having an 'image_id'.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, List[Dict]]: Mapping from image_id to a list of annotations\n",
    "                               belonging to that image.\n",
    "    \"\"\"\n",
    "    annotations_by_image = {}\n",
    "    for ann in annotations['annotations']:\n",
    "        img_id = ann['image_id']\n",
    "        if img_id not in annotations_by_image:\n",
    "            annotations_by_image[img_id] = []\n",
    "        annotations_by_image[img_id].append(ann)\n",
    "    return annotations_by_image\n",
    "\n",
    "\n",
    "def add_gps_to_annotations(annotations: List[Dict], images: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Augment each annotation with GPS data from its corresponding image.\n",
    "\n",
    "    Args:\n",
    "        annotations (List[Dict]): List of annotation dictionaries, each\n",
    "                                  containing an 'image_id' field.\n",
    "        images (List[Dict]): List of image entries (e.g., COCO images)\n",
    "                              each with 'id' and optional 'gps_data'.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: The same list of annotations, modified in place, where\n",
    "                    each annotation has a new 'gps_data' field set to the\n",
    "                    matching image's GPS info or None if unavailable.\n",
    "    \"\"\"\n",
    "    # Build a mapping from image_id to gps_data\n",
    "    image_id_to_gps = {img['id']: img.get('gps_data') for img in images}\n",
    "    for ann in annotations:\n",
    "        ann['gps_data'] = image_id_to_gps.get(ann['image_id'])\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def get_base_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the base filename from a path or URL, stripping query parameters.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Full file path or URL potentially containing query strings.\n",
    "\n",
    "    Returns:\n",
    "        str: The base filename without any directory path or query suffix.\n",
    "    \"\"\"\n",
    "    base = os.path.basename(filename)\n",
    "    return base.split('?')[0]\n",
    "\n",
    "\n",
    "def create_filename_to_id_mapping(coco_data: Dict) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Build a lookup mapping base filenames to COCO image IDs.\n",
    "\n",
    "    Args:\n",
    "        coco_data (Dict): COCO dataset dictionary containing an 'images'\n",
    "                          list of image entries with 'file_name' and 'id'.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: Dictionary mapping each base filename (stripped of path\n",
    "                        and query) to its corresponding image ID.\n",
    "    \"\"\"\n",
    "    return {get_base_filename(img['file_name']): img['id'] \n",
    "            for img in coco_data.get('images', [])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_detections_to_coco(detections_dir: str, output_file: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Convert detection JSON files in a directory to COCO format and save to a file.\n",
    "\n",
    "    Args:\n",
    "        detections_dir (str): Directory containing detection `.json` files.\n",
    "        output_file (str): Path to the output file where COCO-formatted JSON will be saved.\n",
    "\n",
    "    Returns:\n",
    "        Dict: COCO-formatted dataset including 'images', 'annotations', and 'categories' lists.\n",
    "    \"\"\"\n",
    "    print(\"Converting detections to COCO format...\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    coco_data = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": DEFAULT_CATEGORIES\n",
    "    }\n",
    "    \n",
    "    image_id = 1\n",
    "    annotation_id = 1\n",
    "    \n",
    "    for json_file in sorted(Path(detections_dir).glob('*.json')):\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                detection_data = json.load(f)\n",
    "            \n",
    "            image_path = json_file.with_suffix('.jpg')\n",
    "            if not image_path.exists():\n",
    "                print(f\"Warning: Image file not found for {json_file}\")\n",
    "                continue\n",
    "            \n",
    "            # Add image entry\n",
    "            image_entry = create_image_entry(image_id, DEFAULT_WIDTH, DEFAULT_HEIGHT, detection_data)\n",
    "            coco_data[\"images\"].append(image_entry)\n",
    "            \n",
    "            # Process detections\n",
    "            for detection in detection_data.get('detections', []):\n",
    "                bbox = convert_prediction_box(detection)\n",
    "                raw_category_id = detection['object_class']\n",
    "                \n",
    "                if raw_category_id not in DEFAULT_CATEGORY_MAPPING:\n",
    "                    print(f\"Warning: Unknown category ID {raw_category_id}\")\n",
    "                    continue\n",
    "                    \n",
    "                mapped_category_id = DEFAULT_CATEGORY_MAPPING[raw_category_id]\n",
    "                \n",
    "                annotation = {\n",
    "                    \"id\": annotation_id,\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": mapped_category_id,\n",
    "                    \"bbox\": bbox\n",
    "                }\n",
    "                coco_data[\"annotations\"].append(annotation)\n",
    "                annotation_id += 1\n",
    "            \n",
    "            image_id += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save COCO format file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(coco_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Processed {len(coco_data['images'])} images\")\n",
    "    print(f\"Total detections: {len(coco_data['annotations'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_detections_to_coco(detections_dir, processed_detections_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_coco_ids(detections_coco: str, annotations_coco: str, \n",
    "                    output_file: str) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Align image and annotation IDs between two COCO-format JSON files based on matching filenames.\n",
    "\n",
    "    Args:\n",
    "        detections_coco (str): Path to the COCO-format file containing detection data.\n",
    "        annotations_coco (str): Path to the COCO-format file containing ground truth annotations.\n",
    "        output_file (str): Path to the output file where the updated annotations will be saved.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict, Dict]: A tuple containing the original detections dictionary and the updated\n",
    "                           annotations dictionary with aligned IDs and merged metadata.\n",
    "    \"\"\"\n",
    "    print(\"Aligning IDs between COCO files...\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    with open(detections_coco, 'r') as f:\n",
    "        detections = json.load(f)\n",
    "    with open(annotations_coco, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    # Create mappings\n",
    "    detections_filename_to_id = create_filename_to_id_mapping(detections)\n",
    "    annotations_filename_to_id = create_filename_to_id_mapping(annotations)\n",
    "    \n",
    "    # Create ID mapping\n",
    "    id_mapping = {annotations_filename_to_id[filename]: det_id\n",
    "                    for filename, det_id in detections_filename_to_id.items()\n",
    "                    if filename in annotations_filename_to_id}\n",
    "    \n",
    "    # Create detections data mapping\n",
    "    detections_data = {img['id']: {\n",
    "        'gps_data': img['gps_data'],\n",
    "        'record_timestamp': img['record_timestamp']\n",
    "    } for img in detections['images']}\n",
    "    \n",
    "    # Update annotations\n",
    "    for img in annotations['images']:\n",
    "        base_filename = get_base_filename(img['file_name'])\n",
    "        if base_filename in detections_filename_to_id:\n",
    "            new_id = detections_filename_to_id[base_filename]\n",
    "            img['id'] = new_id\n",
    "            if new_id in detections_data:\n",
    "                img['gps_data'] = detections_data[new_id]['gps_data']\n",
    "                img['record_timestamp'] = detections_data[new_id]['record_timestamp']\n",
    "                img['file_name'] = base_filename\n",
    "    \n",
    "    # Update annotation IDs\n",
    "    annotations_by_image = group_annotations_by_image(annotations)\n",
    "    new_annotation_id = 1\n",
    "    \n",
    "    for old_image_id, new_image_id in id_mapping.items():\n",
    "        if old_image_id in annotations_by_image:\n",
    "            for ann in sorted(annotations_by_image[old_image_id], key=lambda x: x['id']):\n",
    "                ann['id'] = new_annotation_id\n",
    "                ann['image_id'] = new_image_id\n",
    "                new_annotation_id += 1\n",
    "    \n",
    "    # Sort and save\n",
    "    annotations['images'] = sorted(annotations['images'], key=lambda x: x['id'])\n",
    "    annotations['annotations'] = sorted(annotations['annotations'], key=lambda x: x['id'])\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(annotations, f, indent=2)\n",
    "    \n",
    "    print(f\"Matched {len(id_mapping)} images\")\n",
    "    print(f\"Total annotations: {len(annotations['annotations'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_coco_ids(processed_detections_file, annotations_file, processed_annotations_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic helper functions\n",
    "\n",
    "def boxes_overlap(box1: List[float], box2: List[float]) -> bool: \n",
    "    \"\"\"Check if two bounding boxes overlap at all.\n",
    "    \n",
    "    Args:\n",
    "        box1 (List[float]): First bounding box in format [x, y, width, height]\n",
    "        box2 (List[float]): Second bounding box in format [x, y, width, height]\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if boxes overlap (intersection area > 0), False otherwise\n",
    "    \"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[0] + box1[2], box2[0] + box2[2])\n",
    "    y2 = min(box1[1] + box1[3], box2[1] + box2[3])\n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    return intersection > 0\n",
    "\n",
    "def filter_detections_to_annotated_images(annotations: List[Dict], \n",
    "                                          detections: List[Dict]) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"Filter detections to only keep those from images that have ground truth annotations.\n",
    "    \n",
    "    Args:\n",
    "        annotations (List[Dict]): List of ground truth annotations\n",
    "        detections (List[Dict]): List of predicted detections\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: A tuple containing:\n",
    "            - annotations: The original annotations (unchanged)\n",
    "            - filtered_detections: Only detections from images that have ground truth annotations\n",
    "    \"\"\"\n",
    "    annotated_image_ids = set(ann['image_id'] for ann in annotations)\n",
    "    filtered_detections = [det for det in detections if det['image_id'] in annotated_image_ids]\n",
    "    return annotations, filtered_detections\n",
    "\n",
    "def group_by_image(annotations: List[Dict], \n",
    "                   detections: List[Dict]) -> Tuple[Dict[int, List[Dict]], Dict[int, List[Dict]], Set[int]]:\n",
    "    \"\"\"Group annotations and detections by image_id.\n",
    "    \n",
    "    Args:\n",
    "        annotations (List[Dict]): List of ground truth annotations\n",
    "        detections (List[Dict]): List of predicted detections\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[Dict[int, List[Dict]], Dict[int, List[Dict]], Set[int]]: A tuple containing:\n",
    "            - ground_truth_by_image: Dict mapping image_id to list of annotations\n",
    "            - predictions_by_image: Dict mapping image_id to list of detections\n",
    "            - annotated_image_ids: Set of image_ids that have ground truth annotations\n",
    "    \"\"\"\n",
    "    ground_truth_by_image = defaultdict(list)\n",
    "    predictions_by_image = defaultdict(list)\n",
    "    \n",
    "    for annotation in annotations:\n",
    "        ground_truth_by_image[annotation['image_id']].append(annotation)\n",
    "    \n",
    "    annotated_image_ids = set(ground_truth_by_image.keys())\n",
    "    for detection in detections:\n",
    "        if detection['image_id'] in annotated_image_ids:\n",
    "            predictions_by_image[detection['image_id']].append(detection)\n",
    "            \n",
    "    return ground_truth_by_image, predictions_by_image, annotated_image_ids\n",
    "\n",
    "def get_classes_in_image(image_objects: List[Dict]) -> Set[int]:\n",
    "    \"\"\"Get set of unique class IDs in an image.\n",
    "    \n",
    "    Args:\n",
    "        image_objects (List[Dict]): List of objects (either annotations or detections)\n",
    "    \n",
    "    Returns:\n",
    "        Set[int]: Set of unique category_ids in the image\n",
    "    \"\"\"\n",
    "    return {obj['category_id'] for obj in image_objects}\n",
    "\n",
    "def filter_by_bbox_size(annotations: List[Dict], detections: List[Dict], \n",
    "                        min_relative_size: float = 0.001) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"Filter annotations and detections to only keep bounding boxes above a minimum relative size.\n",
    "    \n",
    "    Args:\n",
    "        annotations (List[Dict]): List of ground truth annotations\n",
    "        detections (List[Dict]): List of predicted detections\n",
    "        min_relative_size (float): Minimum relative size (as fraction of image) to keep.\n",
    "                                  Default is 0.001 (0.1% of image size)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: A tuple containing:\n",
    "            - filtered_annotations: List of annotations with boxes above size threshold\n",
    "            - filtered_detections: List of detections with boxes above size threshold\n",
    "    \"\"\"\n",
    "    def is_bbox_large_enough(bbox: List[float]) -> bool:\n",
    "        \"\"\"Check if bounding box area is above minimum relative size.\"\"\"\n",
    "        width, height = bbox[2], bbox[3]  # bbox format is [x, y, width, height]\n",
    "        return (width * height) >= min_relative_size\n",
    "\n",
    "    filtered_annotations = [ann for ann in annotations if is_bbox_large_enough(ann['bbox'])]\n",
    "    filtered_detections = [det for det in detections if is_bbox_large_enough(det['bbox'])]\n",
    "    \n",
    "    return filtered_annotations, filtered_detections\n",
    "\n",
    "def cluster_images_by_gps_and_select_per_class(annotations: List[Dict], \n",
    "                                               detections: List[Dict], \n",
    "                                               eps_meters: float = 20, \n",
    "                                               min_samples: int = 1) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"Cluster images by GPS location and select one image per cluster per class.\n",
    "    \n",
    "    Clusters images by GPS location using DBSCAN. For each cluster and each class,\n",
    "    keeps only one image (the one with the most detections of that class).\n",
    "    Filters annotations and detections to only those in the selected images.\n",
    "    \n",
    "    Args:\n",
    "        annotations (List[Dict]): List of ground truth annotations (with gps_data)\n",
    "        detections (List[Dict]): List of predicted detections (with gps_data)\n",
    "        eps_meters (float): DBSCAN epsilon in meters. Default is 20.\n",
    "        min_samples (int): DBSCAN min_samples. Default is 1.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: A tuple containing:\n",
    "            - filtered_annotations: Annotations from selected images\n",
    "            - filtered_detections: Detections from selected images\n",
    "    \"\"\"\n",
    "    # Build image_id to GPS mapping from annotations\n",
    "    image_gps = {}\n",
    "    for ann in annotations:\n",
    "        img_id = ann['image_id']\n",
    "        if img_id not in image_gps and ann.get('gps_data'):\n",
    "            gps = ann['gps_data']\n",
    "            if 'latitude' in gps and 'longitude' in gps:\n",
    "                image_gps[img_id] = (gps['latitude'], gps['longitude'])\n",
    "\n",
    "    image_ids = list(image_gps.keys())\n",
    "    if not image_ids:\n",
    "        return annotations, detections\n",
    "\n",
    "    coords = np.array([image_gps[img_id] for img_id in image_ids])\n",
    "    coords_rad = np.radians(coords)\n",
    "    db = DBSCAN(eps=eps_meters/6371008.8, min_samples=min_samples, metric='haversine')\n",
    "    labels = db.fit_predict(coords_rad)\n",
    "\n",
    "    # Map image_id to cluster label\n",
    "    image_id_to_cluster = {img_id: label for img_id, label in zip(image_ids, labels)}\n",
    "\n",
    "    # For each cluster and class, keep one image (with most detections of that class)\n",
    "    cluster_class_to_images = defaultdict(lambda: defaultdict(list))\n",
    "    image_class_count = defaultdict(lambda: defaultdict(int))\n",
    "    for det in detections:\n",
    "        img_id = det['image_id']\n",
    "        class_id = det['category_id']\n",
    "        if img_id in image_id_to_cluster:\n",
    "            cluster = image_id_to_cluster[img_id]\n",
    "            cluster_class_to_images[cluster][class_id].append(img_id)\n",
    "            image_class_count[img_id][class_id] += 1\n",
    "\n",
    "    selected_image_ids = set()\n",
    "    for cluster, class_to_images in cluster_class_to_images.items():\n",
    "        for class_id, img_ids in class_to_images.items():\n",
    "            best_img_id = max(img_ids, key=lambda img_id: image_class_count[img_id][class_id])\n",
    "            selected_image_ids.add(best_img_id)\n",
    "\n",
    "    filtered_annotations = [ann for ann in annotations if ann['image_id'] in selected_image_ids]\n",
    "    filtered_detections = [det for det in detections if det['image_id'] in selected_image_ids]\n",
    "    return filtered_annotations, filtered_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric calculation helpers\n",
    "\n",
    "def calculate_class_metrics(metrics: Dict[str, int], class_id: int) -> Dict[str, float]:\n",
    "    \"\"\"Calculate precision and recall for a single class.\n",
    "    \n",
    "    Special handling for classes 1 and 2:\n",
    "    - These classes only have false positives and false negatives\n",
    "    - Only recall is calculated and returned\n",
    "    \n",
    "    Args:\n",
    "        metrics (Dict[str, int]): Dictionary containing counts of:\n",
    "            - tp: True positives\n",
    "            - fp: False positives\n",
    "            - fn: False negatives\n",
    "        class_id (int): ID of the class to calculate metrics for\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing:\n",
    "            - For classes 1 and 2: recall, true_positives, false_positives, false_negatives\n",
    "            - For other classes: precision, recall, true_positives, false_positives, false_negatives\n",
    "    \"\"\"\n",
    "    true_positives = metrics['tp']\n",
    "    false_positives = metrics['fp']\n",
    "    false_negatives = metrics['fn']\n",
    "\n",
    "    # For classes 1 and 2, only calculate recall\n",
    "    if class_id in [1, 2]:\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        return {\n",
    "            'recall': recall,\n",
    "            'true_positives': true_positives,\n",
    "            'false_positives': false_positives,\n",
    "            'false_negatives': false_negatives\n",
    "        }\n",
    "\n",
    "    # For other classes, calculate both precision and recall\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'true_positives': true_positives,\n",
    "        'false_positives': false_positives,\n",
    "        'false_negatives': false_negatives\n",
    "    }\n",
    "\n",
    "def process_image_predictions(image_ground_truth: List[Dict], \n",
    "                            image_predictions: List[Dict], \n",
    "                            class_metrics: Dict[int, Dict[str, int]]) -> None:\n",
    "    \"\"\"Process predictions for a single image using binary overlap.\n",
    "    \n",
    "    For each detection:\n",
    "    - If it overlaps with an unmatched ground truth of the same class: count as true positive\n",
    "    - If no overlap: count as false positive\n",
    "    - Unmatched ground truth: count as false negative\n",
    "    \n",
    "    Special handling for classes 1 and 2:\n",
    "    - These classes only have false negative annotations\n",
    "    - All detections are counted as true positives (we found a false negative)\n",
    "    - All ground truth annotations are counted as false negatives (they were missed detections)\n",
    "    \n",
    "    Args:\n",
    "        image_ground_truth (List[Dict]): Ground truth annotations for an image\n",
    "        image_predictions (List[Dict]): Predicted detections for an image\n",
    "        class_metrics (Dict[int, Dict[str, int]]): Dictionary to store metrics per class,\n",
    "            modified in place. Each class has counts for 'tp', 'fp', and 'fn'.\n",
    "    \"\"\"\n",
    "    matched_ground_truth = set()\n",
    "    \n",
    "    for prediction in image_predictions:\n",
    "        prediction_class = prediction['category_id']\n",
    "        prediction_box = prediction['bbox']\n",
    "        matched = False\n",
    "\n",
    "        # For classes 1 and 2, all detections are true positives\n",
    "        if prediction_class in [1, 2]:\n",
    "            class_metrics[prediction_class]['tp'] += 1\n",
    "            continue\n",
    "\n",
    "        for gt_idx, ground_truth in enumerate(image_ground_truth):\n",
    "            if gt_idx in matched_ground_truth:\n",
    "                continue\n",
    "            if ground_truth['category_id'] == prediction_class:\n",
    "                ground_truth_box = ground_truth['bbox']\n",
    "                if boxes_overlap(prediction_box, ground_truth_box):\n",
    "                    class_metrics[prediction_class]['tp'] += 1\n",
    "                    matched_ground_truth.add(gt_idx)\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "        if not matched:\n",
    "            class_metrics[prediction_class]['fp'] += 1\n",
    "\n",
    "    for gt_idx, ground_truth in enumerate(image_ground_truth):\n",
    "        # Special handling for classes 1 and 2, they only have false negative annotations\n",
    "        if ground_truth['category_id'] in [1, 2]:\n",
    "            class_metrics[ground_truth['category_id']]['fn'] += 1\n",
    "        elif gt_idx not in matched_ground_truth:\n",
    "            class_metrics[ground_truth['category_id']]['fn'] += 1\n",
    "\n",
    "def calculate_binary_metrics(annotations: List[Dict], detections: List[Dict]) -> Dict[int, Dict[str, float]]: \n",
    "    \"\"\"Calculate precision and recall using binary overlap (any overlap counts).\n",
    "    \n",
    "    For each class:\n",
    "    - True positive: Detection overlaps with ground truth of same class\n",
    "    - False positive: Detection doesn't overlap with any ground truth of same class\n",
    "    - False negative: Ground truth doesn't overlap with any detection of same class\n",
    "    \n",
    "    Special classes (1 and 2):\n",
    "    - Only recall is calculated\n",
    "    - Recall is calculated as (detections) / (detections + annotations)\n",
    "    \n",
    "    Args:\n",
    "        annotations (List[Dict]): Ground truth annotations\n",
    "        detections (List[Dict]): Predicted detections\n",
    "    \n",
    "    Returns:\n",
    "        Dict[int, Dict[str, float]]: Metrics per class including:\n",
    "            - For classes 1 and 2: recall, true_positives, false_positives, false_negatives\n",
    "            - For other classes: precision, recall, true_positives, false_positives, false_negatives\n",
    "    \"\"\"\n",
    "    ground_truth_by_image, predictions_by_image, annotated_image_ids = group_by_image(annotations, detections)\n",
    "    class_metrics = defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0, 'total_detections': 0, 'total_annotations': 0})\n",
    "\n",
    "    for image_id in annotated_image_ids:\n",
    "        image_ground_truth = ground_truth_by_image[image_id]\n",
    "        image_predictions = predictions_by_image[image_id]\n",
    "        process_image_predictions(image_ground_truth, image_predictions, class_metrics)\n",
    "\n",
    "    results = {}\n",
    "    for class_id, metrics in class_metrics.items():\n",
    "        results[class_id] = calculate_class_metrics(metrics, class_id)\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_image_level_predictions(image_ground_truth: List[Dict], \n",
    "                                  image_predictions: List[Dict], \n",
    "                                  class_metrics: Dict[int, Dict[str, int]]) -> None:\n",
    "    \"\"\"Process image-level predictions where co-existence counts as true positive.\n",
    "    \n",
    "    For each class:\n",
    "    - True positive: Class appears in both ground truth and predictions for this image\n",
    "    - False positive: Class appears only in predictions for this image\n",
    "    - False negative: Class appears only in ground truth for this image\n",
    "    \n",
    "    The counts represent the number of images where each case occurs.\n",
    "    \n",
    "    Args:\n",
    "        image_ground_truth (List[Dict]): Ground truth annotations for an image\n",
    "        image_predictions (List[Dict]): Predicted detections for an image\n",
    "        class_metrics (Dict[int, Dict[str, int]]): Dictionary to store metrics per class,\n",
    "            modified in place. Each class has counts for 'tp', 'fp', and 'fn'.\n",
    "    \"\"\"\n",
    "    gt_classes = get_classes_in_image(image_ground_truth)\n",
    "    pred_classes = get_classes_in_image(image_predictions)\n",
    "    \n",
    "    for class_id in gt_classes & pred_classes:\n",
    "        class_metrics[class_id]['tp'] += 1  # One image counted as true positive\n",
    "    \n",
    "    for class_id in pred_classes - gt_classes:\n",
    "        class_metrics[class_id]['fp'] += 1  # One image counted as false positive\n",
    "    \n",
    "    for class_id in gt_classes - pred_classes:\n",
    "        class_metrics[class_id]['fn'] += 1  # One image counted as false negative\n",
    "\n",
    "def calculate_image_level_metrics(annotations: List[Dict], detections: List[Dict]) -> Dict[int, Dict[str, float]]:\n",
    "    \"\"\"Calculate precision and recall at image level (co-existence counts as true positive).\n",
    "    \n",
    "    For each class:\n",
    "    - True positives: Number of images where the class appears in both ground truth and predictions\n",
    "    - False positives: Number of images where the class appears only in predictions\n",
    "    - False negatives: Number of images where the class appears only in ground truth\n",
    "    \n",
    "    Precision = TP / (TP + FP)  # Proportion of images with predictions that are correct\n",
    "    Recall = TP / (TP + FN)     # Proportion of images with ground truth that are detected\n",
    "    \n",
    "    Args:\n",
    "        annotations (List[Dict]): Ground truth annotations\n",
    "        detections (List[Dict]): Predicted detections\n",
    "    \n",
    "    Returns:\n",
    "        Dict[int, Dict[str, float]]: Metrics per class including:\n",
    "            - For classes 1 and 2: recall, true_positives, false_positives, false_negatives\n",
    "            - For other classes: precision, recall, true_positives, false_positives, false_negatives\n",
    "    \"\"\"\n",
    "    ground_truth_by_image, predictions_by_image, annotated_image_ids = group_by_image(annotations, detections)\n",
    "    class_metrics = defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0})\n",
    "\n",
    "    for image_id in annotated_image_ids:\n",
    "        image_ground_truth = ground_truth_by_image[image_id]\n",
    "        image_predictions = predictions_by_image[image_id]\n",
    "        process_image_level_predictions(image_ground_truth, image_predictions, class_metrics)\n",
    "\n",
    "    results = {}\n",
    "    for class_id, metrics in class_metrics.items():\n",
    "        results[class_id] = calculate_class_metrics(metrics, class_id)\n",
    "\n",
    "    return results\n",
    "\n",
    "def calculate_metrics(annotations: List[Dict], detections: List[Dict]) -> Dict[str, Dict[int, Dict[str, float]]]:\n",
    "    \"\"\"Calculate both binary overlap and image-level metrics.\n",
    "    \n",
    "    Args:\n",
    "        annotations (List[Dict]): Ground truth annotations\n",
    "        detections (List[Dict]): Predicted detections\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Dict[int, Dict[str, float]]]: Dictionary containing:\n",
    "            - binary_overlap: Metrics calculated using binary overlap\n",
    "            - image_level: Metrics calculated at image level\n",
    "    \"\"\"\n",
    "    binary_metrics = calculate_binary_metrics(annotations, detections)\n",
    "    image_level_metrics = calculate_image_level_metrics(annotations, detections)\n",
    "    return {'binary_overlap': binary_metrics, 'image_level': image_level_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and save helpers\n",
    "\n",
    "def load_data() -> Tuple[List[Dict], List[Dict], List[Dict]]:\n",
    "    \"\"\"Load and prepare annotation and detection data for evaluation.\n",
    "    \n",
    "    Loads the processed annotations and detections from JSON files, extracts the relevant\n",
    "    data, and adds GPS information to both annotations and detections.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict], List[Dict]]: A tuple containing:\n",
    "            - annotations: List of ground truth annotations with GPS data\n",
    "            - detections: List of predicted detections with GPS data\n",
    "            - images: List of image metadata from the detections file\n",
    "    \"\"\"\n",
    "    with open('data/output/processed_annotations.json', 'r') as f:\n",
    "        ground_truth_annotations = json.load(f)\n",
    "    with open('data/output/processed_detections.json', 'r') as f:\n",
    "        predicted_detections = json.load(f)\n",
    "    images = predicted_detections.get('images', [])\n",
    "    detections = predicted_detections['annotations']\n",
    "    annotations = ground_truth_annotations['annotations']\n",
    "    \n",
    "    # Add GPS data to both annotations and detections\n",
    "    add_gps_to_annotations(annotations, images)\n",
    "    add_gps_to_annotations(detections, images)\n",
    "    return annotations, detections, images\n",
    "\n",
    "def save_metrics(metrics: Dict[str, Dict[int, Dict[str, float]]], \n",
    "                threshold: float, \n",
    "                mode: str, \n",
    "                output_dir: str) -> None:\n",
    "    \"\"\"Save evaluation metrics to JSON files.\n",
    "    \n",
    "    Creates a directory structure based on the threshold and mode, then saves\n",
    "    both binary overlap and image-level metrics as separate JSON files.\n",
    "    \n",
    "    Args:\n",
    "        metrics (Dict[str, Dict[int, Dict[str, float]]]): Dictionary containing:\n",
    "            - binary_overlap: Metrics calculated using binary overlap\n",
    "            - image_level: Metrics calculated at image level\n",
    "        threshold (float): The bounding box size threshold used for evaluation\n",
    "        mode (str): Either 'unclustered' or 'clustered' to indicate the evaluation mode\n",
    "        output_dir (str): Base directory where metrics will be saved\n",
    "    \"\"\"\n",
    "    threshold_dir = f'{output_dir}/{mode}/threshold_{threshold:.5f}'\n",
    "    os.makedirs(threshold_dir, exist_ok=True)\n",
    "    with open(f'{threshold_dir}/binary_overlap.json', 'w') as f:\n",
    "        json.dump(metrics['binary_overlap'], f, indent=2)\n",
    "    with open(f'{threshold_dir}/image_level.json', 'w') as f:\n",
    "        json.dump(metrics['image_level'], f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = 'data/output/metrics'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "# Define thresholds\n",
    "thresholds = np.arange(0, 0.10 + 0.00005, 0.00005)\n",
    "\n",
    "# Run metrics for each threshold\n",
    "eps_meters = 5\n",
    "unclustered_binary_metrics_data = []\n",
    "unclustered_image_metrics_data = []\n",
    "clustered_binary_metrics_data = []\n",
    "clustered_image_metrics_data = []\n",
    "\n",
    "# Load data\n",
    "all_annotations, all_detections, all_images = load_data()\n",
    "\n",
    "for threshold in thresholds:\n",
    "    print(f\"Processing threshold: {threshold:.5f}\")\n",
    "    # Filter by bbox size\n",
    "    annotations, detections = filter_by_bbox_size(all_annotations, all_detections, threshold)\n",
    "    # Filter out detection images to only keep annotated images\n",
    "    annotations, detections = filter_detections_to_annotated_images(annotations, detections)\n",
    "    # Cluster\n",
    "    clustered_annotations, clustered_detections = cluster_images_by_gps_and_select_per_class(annotations, detections, eps_meters=eps_meters)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(annotations, detections)\n",
    "    clustered_metrics = calculate_metrics(clustered_annotations, clustered_detections)\n",
    "    # Save metrics\n",
    "    save_metrics(metrics, threshold, 'unclustered', output_dir)\n",
    "    save_metrics(clustered_metrics, threshold, 'clustered', output_dir)\n",
    "\n",
    "    # Collect metrics for plotting\n",
    "    unclustered_binary_metrics_data.append(metrics['binary_overlap'])\n",
    "    unclustered_image_metrics_data.append(metrics['image_level'])\n",
    "    clustered_binary_metrics_data.append(clustered_metrics['binary_overlap'])\n",
    "    clustered_image_metrics_data.append(clustered_metrics['image_level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot helpers\n",
    "\n",
    "def plot_metrics(thresholds: List[float], \n",
    "                metrics_data: List[Dict[int, Dict[str, float]]], \n",
    "                metric_type: str, \n",
    "                output_dir: str) -> None:\n",
    "    \"\"\"Create plots showing precision and recall trends for each class.\n",
    "    \n",
    "    For each class, creates a line plot showing how precision and recall change\n",
    "    with different bounding box size thresholds. Classes 1 and 2 only show recall\n",
    "    since they don't have precision metrics.\n",
    "    \n",
    "    Args:\n",
    "        thresholds (List[float]): List of bounding box size thresholds used for evaluation\n",
    "        metrics_data (List[Dict[int, Dict[str, float]]]): List of metric dictionaries, one per threshold.\n",
    "            Each dictionary maps class IDs to their metrics, which may include:\n",
    "            - precision: Precision score (not present for classes 1 and 2)\n",
    "            - recall: Recall score\n",
    "        metric_type (str): Type of metrics being plotted (e.g. 'binary_overlap' or 'image_level')\n",
    "        output_dir (str): Directory where plot images will be saved\n",
    "    \n",
    "    The function creates one plot per class, saved as PNG files with names like:\n",
    "    'class_1_binary_overlap.png', 'class_2_image_level.png', etc.\n",
    "    \"\"\"\n",
    "    # Get all class IDs that exist in the data\n",
    "    all_class_ids = set()\n",
    "    for metrics in metrics_data:\n",
    "        all_class_ids.update(metrics.keys())\n",
    "    class_ids = sorted(all_class_ids)\n",
    "\n",
    "    for class_id in class_ids:\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "\n",
    "        for metrics in metrics_data:\n",
    "            if class_id in metrics:\n",
    "                recalls.append(metrics[class_id].get('recall', 0))\n",
    "                # Only append precision if it exists\n",
    "                if 'precision' in metrics[class_id]:\n",
    "                    precisions.append(metrics[class_id]['precision'])\n",
    "                else:\n",
    "                    precisions.append(None)\n",
    "            else:\n",
    "                recalls.append(0)\n",
    "                precisions.append(None)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(thresholds, recalls, 'r-', label='Recall')\n",
    "        if any(p is not None for p in precisions):\n",
    "            # Only plot precision if it exists for this class\n",
    "            plt.plot(thresholds, [p if p is not None else 0 for p in precisions], 'b-', label='Precision')\n",
    "\n",
    "        plt.title(f'Class {class_id} - {metric_type} Metrics')\n",
    "        plt.xlabel('Bounding Box Size Threshold')\n",
    "        plt.ylabel('Score')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{output_dir}/class_{class_id}_{metric_type}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = 'data/output/plots'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Plot results\n",
    "plot_metrics(thresholds, unclustered_binary_metrics_data, 'binary_overlap (unclustered)', output_dir)\n",
    "plot_metrics(thresholds, clustered_binary_metrics_data, 'binary_overlap (clustered)', output_dir)\n",
    "plot_metrics(thresholds, unclustered_image_metrics_data, 'image_level (unclustered)', output_dir)\n",
    "plot_metrics(thresholds, clustered_image_metrics_data, 'image_level (clustered)', output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create HTML report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Paths\n",
    "METRICS_DIR = 'data/output/metrics'\n",
    "PLOTS_DIR = 'data/output/plots'\n",
    "IMAGES_DIR = 'data/input/raw_detections'\n",
    "ANNOTATED_DIR = 'data/output/annotated_examples'\n",
    "REPORT_PATH = 'data/output/report.html'\n",
    "\n",
    "# Classes\n",
    "CLASSES = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image_path: str, boxes: List[List[float]], color: Tuple[int, int, int], label: Optional[str] = None) -> np.ndarray:\n",
    "    \"\"\"Draw bounding boxes on an image.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the input image\n",
    "        boxes (List[List[float]]): List of bounding boxes in format [[x, y, width, height], ...]\n",
    "        color (Tuple[int, int, int]): BGR color tuple for the boxes\n",
    "        label (Optional[str]): Optional label to display above each box\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Image with drawn boxes\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    for box in boxes:\n",
    "        x, y, w, h = box\n",
    "        h_img, w_img = img.shape[:2]\n",
    "        pt1 = (int(x * w_img), int(y * h_img))\n",
    "        pt2 = (int((x + w) * w_img), int((y + h) * h_img))\n",
    "        cv2.rectangle(img, pt1, pt2, color, 2)\n",
    "        if label:\n",
    "            cv2.putText(img, label, (pt1[0], pt1[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "    return img\n",
    "\n",
    "def save_annotated_example(image_id: int, image_info: Dict[str, Any], class_id: int, \n",
    "                         fp_boxes: List[List[float]], fn_boxes: List[List[float]], \n",
    "                         prefix: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Save annotated images showing false positives and false negatives.\n",
    "    \n",
    "    Args:\n",
    "        image_id (int): ID of the image\n",
    "        image_info (Dict[str, Any]): Dictionary containing image information including file_name\n",
    "        class_id (int): ID of the class being evaluated\n",
    "        fp_boxes (List[List[float]]): List of false positive bounding boxes\n",
    "        fn_boxes (List[List[float]]): List of false negative bounding boxes\n",
    "        prefix (str): Prefix for output filenames\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[Optional[str], Optional[str]]: Paths to saved FP and FN images, or None if no boxes\n",
    "    \"\"\"\n",
    "    file_name = image_info['file_name']\n",
    "    img_path = os.path.join(IMAGES_DIR, file_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return None, None\n",
    "    # Draw FPs in red, FNs in blue\n",
    "    img_fp = draw_boxes(img_path, fp_boxes, (0,0,255), 'FP') if fp_boxes else img.copy()\n",
    "    img_fn = draw_boxes(img_path, fn_boxes, (255,0,0), 'FN') if fn_boxes else img.copy()\n",
    "    out_fp = f'{ANNOTATED_DIR}/{prefix}_class{class_id}_img{image_id}_fp.jpg'\n",
    "    out_fn = f'{ANNOTATED_DIR}/{prefix}_class{class_id}_img{image_id}_fn.jpg'\n",
    "    if fp_boxes:\n",
    "        cv2.imwrite(out_fp, img_fp)\n",
    "    if fn_boxes:\n",
    "        cv2.imwrite(out_fn, img_fn)\n",
    "    return out_fp if fp_boxes else None, out_fn if fn_boxes else None\n",
    "\n",
    "def find_fp_fn(ann_by_img_cls: Dict[int, Dict[int, List[Dict[str, Any]]]], \n",
    "               det_by_img_cls: Dict[int, Dict[int, List[Dict[str, Any]]]], \n",
    "               class_id: int, \n",
    "               max_examples: int = 9) -> Tuple[List[Tuple[int, List[float]]], List[Tuple[int, List[float]]]]:\n",
    "    \"\"\"Find false positive and false negative examples for a given class.\n",
    "    \n",
    "    Args:\n",
    "        ann_by_img_cls (Dict[int, Dict[int, List[Dict[str, Any]]]]): Ground truth annotations by image and class\n",
    "        det_by_img_cls (Dict[int, Dict[int, List[Dict[str, Any]]]]): Detections by image and class\n",
    "        class_id (int): ID of the class to evaluate\n",
    "        max_examples (int): Maximum number of examples to return for each type\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[List[Tuple[int, List[float]]], List[Tuple[int, List[float]]]]: Lists of (image_id, box) tuples for FPs and FNs\n",
    "    \"\"\"\n",
    "    fp_examples = []\n",
    "    fn_examples = []\n",
    "    for image_id in ann_by_img_cls:\n",
    "        gt_boxes = [a['bbox'] for a in ann_by_img_cls[image_id][class_id]]\n",
    "        det_boxes = [d['bbox'] for d in det_by_img_cls[image_id][class_id]]\n",
    "        # FNs: GT box not matched by any detection\n",
    "        for gt in gt_boxes:\n",
    "            matched = any(boxes_overlap(gt, det) for det in det_boxes)\n",
    "            if not matched:\n",
    "                fn_examples.append((image_id, gt))\n",
    "        # FPs: Det box not matched by any GT\n",
    "        for det in det_boxes:\n",
    "            matched = any(boxes_overlap(det, gt) for gt in gt_boxes)\n",
    "            if not matched:\n",
    "                fp_examples.append((image_id, det))\n",
    "    # Randomly sample up to max_examples\n",
    "    if len(fp_examples) > max_examples:\n",
    "        fp_examples = random.sample(fp_examples, max_examples)\n",
    "    if len(fn_examples) > max_examples:\n",
    "        fn_examples = random.sample(fn_examples, max_examples)\n",
    "    return fp_examples, fn_examples\n",
    "\n",
    "def img_to_base64(img_path: str) -> str:\n",
    "    \"\"\"Convert an image file to base64 string.\n",
    "    \n",
    "    Args:\n",
    "        img_path (str): Path to the image file\n",
    "    \n",
    "    Returns:\n",
    "        str: Base64 encoded string of the image\n",
    "    \"\"\"\n",
    "    with open(img_path, 'rb') as img_f:\n",
    "        return base64.b64encode(img_f.read()).decode('utf-8')\n",
    "\n",
    "def plot_to_base64(plot_path: str) -> str:\n",
    "    \"\"\"Convert a plot file to base64 string.\n",
    "    \n",
    "    Args:\n",
    "        plot_path (str): Path to the plot file\n",
    "    \n",
    "    Returns:\n",
    "        str: Base64 encoded string of the plot\n",
    "    \"\"\"\n",
    "    with open(plot_path, 'rb') as img_f:\n",
    "        return base64.b64encode(img_f.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for new annotation\n",
    "os.makedirs(ANNOTATED_DIR, exist_ok=True)\n",
    "\n",
    "# Load class names from processed_annotations.json\n",
    "with open('data/output/processed_annotations.json') as f:\n",
    "    gt_data = json.load(f)\n",
    "category_id_to_name = {cat['id']: cat['name'] for cat in gt_data['categories']}\n",
    "\n",
    "# Load binary metrics at threshold 0\n",
    "with open(f'{METRICS_DIR}/unclustered/threshold_0.00000/binary_overlap.json') as f:\n",
    "    unclustered_metrics = json.load(f)\n",
    "with open(f'{METRICS_DIR}/clustered/threshold_0.00000/binary_overlap.json') as f:\n",
    "    clustered_metrics = json.load(f)\n",
    "\n",
    "# Load detections\n",
    "with open('data/output/processed_detections.json') as f:\n",
    "    det_data = json.load(f)\n",
    "\n",
    "detections = det_data['annotations']\n",
    "images_info = {img['id']: img for img in det_data['images']}\n",
    "annotations = gt_data['annotations']\n",
    "\n",
    "# Group by image_id and class\n",
    "ann_by_img_cls = defaultdict(lambda: defaultdict(list))\n",
    "det_by_img_cls = defaultdict(lambda: defaultdict(list))\n",
    "for ann in annotations:\n",
    "    ann_by_img_cls[ann['image_id']][ann['category_id']].append(ann)\n",
    "for det in detections:\n",
    "    det_by_img_cls[det['image_id']][det['category_id']].append(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML header\n",
    "html = ['<html><head><title>Detection Metrics Report</title>\\\n",
    "<style>\\\n",
    "body { font-family: \"Segoe UI\", Arial, sans-serif; background: #f8f9fa; color: #222; margin: 0; padding: 0 0 40px 0; }\\\n",
    "h1 { background: #2d6cdf; color: #fff; margin: 0 0 24px 0; padding: 24px 0 16px 32px; font-size: 2.2em; letter-spacing: 1px; }\\\n",
    "h2 { color: #2d6cdf; border-bottom: 2px solid #e3e6ea; padding-bottom: 4px; margin-top: 36px; margin-bottom: 12px; }\\\n",
    "h3 { color: #1a3a6b; margin-top: 28px; margin-bottom: 8px; }\\\n",
    "p, b { font-size: 1.08em; }\\\n",
    "table { border-collapse: collapse; margin: 24px 0; background: #fff; box-shadow: 0 2px 8px #0001; }\\\n",
    "th, td { border: 1px solid #e3e6ea; padding: 10px 18px; text-align: center; font-size: 1.05em; }\\\n",
    "th { background: #e3e6ea; color: #1a3a6b; }\\\n",
    "tr:nth-child(even) { background: #f4f6fa; }\\\n",
    "tr:hover { background: #eaf1fb; }\\\n",
    "div[style*=\"display:flex\"] { gap: 16px; flex-wrap: wrap; margin-bottom: 18px; }\\\n",
    "img { border-radius: 8px; box-shadow: 0 2px 8px #0002; transition: transform 0.2s, box-shadow 0.2s; background: #fff; }\\\n",
    "img:hover { transform: scale(1.04); box-shadow: 0 4px 16px #0003; }\\\n",
    "</style></head><body>']\n",
    "html.append('<h1>Detection Metrics Report</h1>')\n",
    "html.append('<p><b>Bounding box size threshold</b>:  The minimum relative area (as a fraction of the image) that a bounding box must have to be included in the evaluation.</p>')\n",
    "html.append('<p><b>Unclustered:</b> metrics are computed on the raw unclustered detections.</p>')\n",
    "html.append('<p><b>Clustered:</b> metrics are computed after grouping images by GPS proximity, and only keeping the image with the most confident detection per object. This removes false negatives for the same object in different images. We use a 5m radius for clustering.</p>')\n",
    "html.append('<p><b>Evaluation metric:</b> Binary overlap, which means that a detection is considered a true positive if its bounding box overlaps with a ground truth bounding box of the same class. Detections that do not overlap with any ground truth are counted as false positives, and ground truth boxes not overlapped by any detection are counted as false negatives.</p>')\n",
    "\n",
    "# Section: Classes 1 and 2\n",
    "for class_id in [1,2]:\n",
    "    class_name = category_id_to_name[class_id]\n",
    "    recall = unclustered_metrics[str(class_id)]['recall']\n",
    "    html.append(f'<h2>{class_name.title()}</h2>')\n",
    "    html.append(f'<b>Recall (size threshold 0, unclustered):</b> {recall:.4f}<br>')\n",
    "    # Annotated examples\n",
    "    html.append('<h3>False Positives</h3>')\n",
    "    html.append('<p>Not shown: For this class, only false negatives are annotated, so false positives are not available for visualization.</p>')\n",
    "    fp_examples, fn_examples = find_fp_fn(ann_by_img_cls, det_by_img_cls, class_id)\n",
    "    html.append(f'<h3>False Negatives</h3><div style=\"display:flex;flex-wrap:wrap;\">')\n",
    "    for idx, (img_id, box) in enumerate(fn_examples):\n",
    "        img_info = images_info[img_id]\n",
    "        _, fn_path = save_annotated_example(img_id, img_info, class_id, [], [box], f'class{class_id}_fn{idx}')\n",
    "        if fn_path:\n",
    "            img_b64 = img_to_base64(fn_path)\n",
    "            html.append(f'<img src=\"data:image/jpeg;base64,{img_b64}\" width=\"320\" style=\"margin:5px;\">')\n",
    "    html.append('</div>')\n",
    "\n",
    "# Section: Classes 3, 4, 5\n",
    "html.append('<h2>Precision and Recall (size threshold 0)</h2>')\n",
    "html.append('<table border=\"1\"><tr><th>Class</th><th>Precision (Unclustered)</th><th>Recall (Unclustered)</th><th>Precision (Clustered)</th><th>Recall (Clustered)</th></tr>')\n",
    "for class_id in [3,4,5]:\n",
    "    class_name = category_id_to_name[class_id]\n",
    "    u = unclustered_metrics[str(class_id)]\n",
    "    c = clustered_metrics[str(class_id)]\n",
    "    html.append(f'<tr><td>{class_name.title()}</td><td>{u.get(\"precision\",\"-\"):.4f}</td><td>{u[\"recall\"]:.4f}</td><td>{c.get(\"precision\",\"-\"):.4f}</td><td>{c[\"recall\"]:.4f}</td></tr>')\n",
    "html.append('</table>')\n",
    "\n",
    "# Plots\n",
    "for class_id in [3,4,5]:\n",
    "    class_name = category_id_to_name[class_id]\n",
    "    html.append(f'<h3>{class_name.title()} Precision/Recall Plot (Unclustered)</h3>')\n",
    "    plot_path = f'{PLOTS_DIR}/class_{class_id}_binary_overlap (unclustered).png'\n",
    "    plot_b64 = plot_to_base64(plot_path)\n",
    "    html.append(f'<img src=\"data:image/png;base64,{plot_b64}\" width=\"600\">')\n",
    "    html.append(f'<h3>{class_name.title()} Precision/Recall Plot (Clustered)</h3>')\n",
    "    plot_path = f'{PLOTS_DIR}/class_{class_id}_binary_overlap (clustered).png'\n",
    "    plot_b64 = plot_to_base64(plot_path)\n",
    "    html.append(f'<img src=\"data:image/png;base64,{plot_b64}\" width=\"600\">')\n",
    "\n",
    "# Annotated examples for 3,4,5\n",
    "for class_id in [3,4,5]:\n",
    "    class_name = category_id_to_name[class_id]\n",
    "    html.append(f'<h3>{class_name.title()} False Positives</h3><div style=\"display:flex;flex-wrap:wrap;\">')\n",
    "    fp_examples, fn_examples = find_fp_fn(ann_by_img_cls, det_by_img_cls, class_id)\n",
    "    for idx, (img_id, box) in enumerate(fp_examples):\n",
    "        img_info = images_info[img_id]\n",
    "        fp_path, _ = save_annotated_example(img_id, img_info, class_id, [box], [], f'class{class_id}_fp{idx}')\n",
    "        if fp_path:\n",
    "            img_b64 = img_to_base64(fp_path)\n",
    "            html.append(f'<img src=\"data:image/jpeg;base64,{img_b64}\" width=\"320\" style=\"margin:5px;\">')\n",
    "    html.append('</div>')\n",
    "    html.append(f'<h3>{class_name.title()} False Negatives</h3><div style=\"display:flex;flex-wrap:wrap;\">')\n",
    "    for idx, (img_id, box) in enumerate(fn_examples):\n",
    "        img_info = images_info[img_id]\n",
    "        _, fn_path = save_annotated_example(img_id, img_info, class_id, [], [box], f'class{class_id}_fn{idx}')\n",
    "        if fn_path:\n",
    "            img_b64 = img_to_base64(fn_path)\n",
    "            html.append(f'<img src=\"data:image/jpeg;base64,{img_b64}\" width=\"320\" style=\"margin:5px;\">')\n",
    "    html.append('</div>')\n",
    "\n",
    "html.append('</body></html>')\n",
    "\n",
    "with open(REPORT_PATH, 'w') as f:\n",
    "    f.write('\\n'.join(html))\n",
    "\n",
    "print(f'Report generated: {REPORT_PATH}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
