{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Minimisation Experiment\n",
    "\n",
    "Code for the dataminimisation experiment to compare different blurring scenarios.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "To prepare the data for the experiment, download from the landingzone the detections and images in the following structure:\n",
    "\n",
    "- `input_folder /`\n",
    "  - `images`\n",
    "  - `detections`\n",
    "\n",
    "Run this notebook to generate the images for the different scenarios which will be saved in `output_folder`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from objectherkenning_openbare_ruimte.data_minimisation.data_minimisation import DataMinimisation\n",
    "\n",
    "# Set appropriate paths\n",
    "input_folder = \"../datasets/oor/data-minimisation/prep_data\"\n",
    "images_folder = os.path.join(input_folder, \"images\")\n",
    "detections_folder =  os.path.join(input_folder, \"detections\")\n",
    "\n",
    "output_folder = \"../datasets/oor/data-minimisation/blur_test\"\n",
    "\n",
    "# Folder to store detections in YOLO format\n",
    "annotations_folder = os.path.join(input_folder, \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert detections from \"landingzone format\" back to YOLO annotation files\n",
    "\n",
    "df = pd.concat([pd.read_csv(os.path.join(detections_folder, file)) for file in os.listdir(detections_folder) if file.endswith(\".csv\")])\n",
    "images = [file for file in os.listdir(images_folder) if file.endswith(\".jpg\")]\n",
    "df = df[df[\"image_name\"].isin(images)]\n",
    "\n",
    "os.makedirs(annotations_folder, exist_ok=True)\n",
    "\n",
    "for image in images:\n",
    "    out_file = os.path.join(annotations_folder, f\"{os.path.splitext(image)[0]}.txt\")\n",
    "    image_df = df[df[\"image_name\"] == image].set_index(\"image_name\")\n",
    "    image_df.to_csv(out_file, sep=\" \", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images for scenarios\n",
    "\n",
    "data_minimisation = DataMinimisation()\n",
    "\n",
    "data_minimisation.process_folder(images_folder, annotations_folder, output_folder, image_format=\"jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Analysis of the landing zone images based on the different scenarios and the annotated false negatives.\n",
    "\n",
    "### Scenario A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from cvtoolkit.converters.azure_coco_to_coco_converter import AzureCocoToCocoConverter\n",
    "\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.metrics.metrics_utils import ObjectClass\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.metrics.per_pixel_stats import PerPixelEvaluator\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.source.oor_evaluation import tba_result_to_df\n",
    "from objectherkenning_openbare_ruimte.performance_evaluation_pipeline.source.yolo_to_coco import convert_yolo_predictions_to_coco_json\n",
    "\n",
    "\n",
    "# Experiment folder\n",
    "exp_dir = \"../datasets/oor/data-minimisation/exp_241118\"\n",
    "\n",
    "# Folder with annotations from on-edge blurring (YOLO format)\n",
    "blurring_annotations_dir = os.path.join(exp_dir, \"blurring_labels_241118\")\n",
    "\n",
    "# JSON file with false negatives from Azure Data Labelling project\n",
    "fn_annotations_file = os.path.join(exp_dir, \"dataminimisation_fn.json\")\n",
    "\n",
    "image_shape = (1280, 720)\n",
    "\n",
    "signals = [\n",
    "    \"26-D18M11Y2024-H12M35S49-03303\",\n",
    "    \"23-D18M11Y2024-H13M50S52-02875\",\n",
    "    \"23-D18M11Y2024-H13M50S52-02904\",\n",
    "    \"56-D18M11Y2024-H10M48S43-06929\",\n",
    "    \"40-D18M11Y2024-H10M32S22-04948\",\n",
    "    \"20-D18M11Y2024-H13M47S49-02501\",\n",
    "    \"7-D18M11Y2024-H09M58S38-00978\",\n",
    "    \"4-D18M11Y2024-H12M13S18-00601\",\n",
    "    \"13-D18M11Y2024-H14M56S50-01636\",\n",
    "    \"9-D18M11Y2024-H14M52S45-01129\",\n",
    "]\n",
    "\n",
    "signals_only = False\n",
    "\n",
    "n_images = len(signals) if signals_only else 462\n",
    "\n",
    "\n",
    "# Convert YOLO blurring annotations to COCO json\n",
    "coco_blurred_file = os.path.join(exp_dir, \"coco_blurred.json\")\n",
    "\n",
    "out_file = convert_yolo_predictions_to_coco_json(\n",
    "    predictions_dir=blurring_annotations_dir,\n",
    "    image_shape=image_shape,\n",
    "    labels_rel_path=\"\",\n",
    "    splits=None,\n",
    "    output_dir=exp_dir\n",
    ")[0]\n",
    "os.rename(out_file, coco_blurred_file)\n",
    "\n",
    "# Convert AML annotations to COCO json\n",
    "coco_fn_file = os.path.join(exp_dir, \"coco_fn.json\")\n",
    "converter = AzureCocoToCocoConverter(\n",
    "    azureml_file=fn_annotations_file,\n",
    "    output_file=coco_fn_file,\n",
    "    new_width=image_shape[0],\n",
    "    new_height=image_shape[1]\n",
    ")\n",
    "converter.convert()\n",
    "\n",
    "# Merge blurring annotations and fn annotations as proxy for ground truth\n",
    "with open(coco_blurred_file, 'r') as f:\n",
    "    coco_blurred_json_content = json.load(f)\n",
    "with open(coco_fn_file, 'r') as f:\n",
    "    coco_fn_json_content = json.load(f)\n",
    "\n",
    "coco_fn_json_content[\"annotations\"] = [ann for ann in coco_fn_json_content[\"annotations\"] if ann[\"category_id\"] != 2]\n",
    "\n",
    "if signals_only:\n",
    "    coco_blurred_json_content = [ann for ann in coco_blurred_json_content if ann[\"image_id\"] in signals]\n",
    "    coco_fn_json_content[\"annotations\"] = [ann for ann in coco_fn_json_content[\"annotations\"] if ann[\"image_id\"] in signals]\n",
    "\n",
    "    with open(coco_blurred_file, \"w\") as f:\n",
    "        json.dump(coco_blurred_json_content, f)\n",
    "\n",
    "with open(coco_fn_file, \"w\") as f:\n",
    "    json.dump(coco_fn_json_content, f)\n",
    "\n",
    "coco_all_json_content = [*coco_blurred_json_content, *coco_fn_json_content[\"annotations\"]]\n",
    "\n",
    "# Remove score if it exists (to prevent issues later on, since AML annotations don't have a score)\n",
    "for annotation in coco_all_json_content:\n",
    "    annotation.pop(\"score\", 0.0)\n",
    "\n",
    "coco_all_file = os.path.join(exp_dir, \"coco_all.json\")\n",
    "\n",
    "with open(coco_all_file, \"w\") as f:\n",
    "    json.dump(coco_all_json_content, f)\n",
    "\n",
    "del coco_all_json_content, coco_blurred_json_content\n",
    "\n",
    "# Evaluate results\n",
    "evaluator = PerPixelEvaluator(\n",
    "    ground_truth_path=coco_all_file,\n",
    "    predictions_path=coco_blurred_file,\n",
    "    image_shape=image_shape,\n",
    ")\n",
    "result = {\n",
    "    \"OOR-v2.2_all\": evaluator.collect_results_per_class_and_size(\n",
    "        classes=[ObjectClass.person, ObjectClass.license_plate],\n",
    "        single_size_only=False\n",
    "    )\n",
    "}\n",
    "\n",
    "result_df = tba_result_to_df(results=result)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pixels = image_shape[0] * image_shape[1] * n_images\n",
    "print(f\"Number of images: {n_images}\")\n",
    "print(f\"Total number of pixels: {total_pixels}\")\n",
    "\n",
    "percent_missed = (result_df[\"False Negatives\"] / total_pixels) * 100\n",
    "\n",
    "print(\"PERSON - percentage of pixels unblurred: \"\n",
    "      f\"{percent_missed.loc[0]:.3f}% ({int(result_df.loc[0, \"False Negatives\"] / n_images)} pixels per image)\")\n",
    "print(\"LICENSE PLATE - percentage of pixels unblurred: \"\n",
    "      f\"{percent_missed.loc[4]:.3f}% ({int(result_df.loc[4, \"False Negatives\"] / n_images)} pixels per image)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario B & C\n",
    "\n",
    "The statistics for scenarios B and C are the same, since the only difference is whether we blur the rest or crop the rest. The number of unblurred pixels is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def yolo_box_to_coco_box(yolo_box, img_shape):\n",
    "    xcn, ycn, wn, hn = yolo_box\n",
    "    xc, w = xcn * img_shape[0], wn * img_shape[0]\n",
    "    yc, h = ycn * img_shape[1], hn * img_shape[1]\n",
    "    x = xc - w / 2\n",
    "    y = yc - h / 2\n",
    "    return [x, y, w, h]\n",
    "\n",
    "def box_to_mask(mask, bbox, box_padding=25):\n",
    "    x_min, y_min, w, h = map(int, bbox)\n",
    "    x_max = x_min + w\n",
    "    y_max = y_min + h\n",
    "\n",
    "    x_min = max(0, x_min - box_padding)\n",
    "    y_min = max(0, y_min - box_padding)\n",
    "    x_max = min(mask.shape[1], x_max + box_padding)\n",
    "    y_max = min(mask.shape[0], y_max + box_padding)\n",
    "\n",
    "    mask[y_min:y_max, x_min:x_max] = 1\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Get container detections from detection_metadata in \"landingzone format\"\n",
    "detections_folder = os.path.join(exp_dir, \"detection_metadata_241118\")\n",
    "detection_df = pd.concat([pd.read_csv(os.path.join(detections_folder, file)) for file in os.listdir(detections_folder) if file.endswith(\".csv\")])\n",
    "detection_df[\"image_name\"] = detection_df[\"image_name\"].str.replace(\".jpg\", \"\")\n",
    "\n",
    "# Padding around containers when blurring / cropping. Default: 25\n",
    "box_padding = 25\n",
    "\n",
    "target_classes = [0, 1]\n",
    "\n",
    "scenario_results = {\n",
    "    \"image\": [],\n",
    "    \"fn_visible\": {\n",
    "        0: [],\n",
    "        1: [],\n",
    "    }\n",
    "}\n",
    "\n",
    "if signals_only:\n",
    "    image_list = signals\n",
    "else:\n",
    "    image_list = detection_df[\"image_name\"].unique()\n",
    "\n",
    "for image in image_list:\n",
    "    scenario_results[\"image\"].append(image)\n",
    "\n",
    "    detection_mask = np.zeros(shape=image_shape[::-1], dtype=\"bool\")\n",
    "    \n",
    "    detections = detection_df[detection_df[\"image_name\"] == f\"{image}\"]\n",
    "    for idx, row in detections.iterrows():\n",
    "        bbox = yolo_box_to_coco_box(row.loc[[\"x_center\", \"y_center\", \"width\", \"height\"]].to_list(), image_shape)\n",
    "        detection_mask = box_to_mask(detection_mask, bbox, box_padding=box_padding)\n",
    "\n",
    "    fns = [ann for ann in coco_fn_json_content[\"annotations\"] if ann[\"image_id\"] == image]\n",
    "\n",
    "    for target in target_classes:\n",
    "        fn_mask = np.zeros(shape=image_shape[::-1], dtype=\"bool\")\n",
    "        for fn in fns:\n",
    "            if fn[\"category_id\"] == target:\n",
    "                fn_mask = box_to_mask(fn_mask, fn[\"bbox\"], box_padding=0)\n",
    "        scenario_results[\"fn_visible\"][target].append(np.count_nonzero(np.logical_and(detection_mask, fn_mask)))\n",
    "px_vis_person = sum(scenario_results[\"fn_visible\"][0])\n",
    "px_vis_license = sum(scenario_results[\"fn_visible\"][1])\n",
    "\n",
    "print(f\"Padding: {box_padding}px\")\n",
    "print(f\"PERSON - pixels visible: {px_vis_person} ({int(px_vis_person / n_images)} per image)\")\n",
    "print(f\"LICENSE PLATE - pixels visible: {px_vis_license} ({int(px_vis_license / n_images)} per image)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ann for ann in coco_fn_json_content[\"annotations\"] if ann[\"image_id\"] == \"26-D18M11Y2024-H12M35S49-03303\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "annotation_files = list(pathlib.Path(blurring_annotations_dir).glob(\"*.txt\"))\n",
    "\n",
    "counts = {\n",
    "    \"0\": 0,\n",
    "    \"1\": 0,\n",
    "}\n",
    "\n",
    "sizes = {\n",
    "    \"0\": [],\n",
    "    \"1\": [],\n",
    "}\n",
    "\n",
    "for file in annotation_files:\n",
    "    with open(file, 'r') as f:\n",
    "        for annotation in f.readlines():\n",
    "            cls_id, x, y, w, h = annotation.split(sep=\" \")[0:5]\n",
    "            counts[cls_id] += 1\n",
    "            sizes[cls_id].append(float(w) * float(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fn_annotations_file, 'r') as f:\n",
    "    json_content = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_unblurred = {\n",
    "    \"0\": 0,\n",
    "    \"1\": 0,\n",
    "    \"2\": 0,\n",
    "}\n",
    "\n",
    "sizes_unblurred = {\n",
    "    \"0\": [],\n",
    "    \"1\": [],\n",
    "}\n",
    "\n",
    "for ann in json_content[\"annotations\"]:\n",
    "    counts_unblurred[str(ann[\"category_id\"] - 1)] += 1\n",
    "    if ann[\"category_id\"] != 3:\n",
    "        bbox = ann[\"bbox\"]\n",
    "        sizes_unblurred[str(ann[\"category_id\"] - 1)].append(bbox[2] * bbox[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_unblurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "bins = np.linspace(0, 0.015, 150)\n",
    "\n",
    "plt.hist(sizes[\"1\"], bins, alpha=0.5, label='Blurred')\n",
    "plt.hist(sizes_unblurred[\"1\"], bins, alpha=0.5, label='Unblurred')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 0.05, 150)\n",
    "\n",
    "plt.hist(sizes[\"0\"], bins, alpha=0.5, label='Blurred')\n",
    "plt.hist(sizes_unblurred[\"0\"], bins, alpha=0.5, label='Unblurred')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oor_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
